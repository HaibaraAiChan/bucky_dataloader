main start at this time 1712916830.9814763
-----------------------------------------before load data 
 Nvidia-smi: 0.3560791015625 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

#nodes: 2449029
#edges: 123718024
#classes: 47
success----------------------------------------
196571
39255
2164782
# Nodes: 2400608
# Edges: 123718024
# Train: 196571
# Val: 39255
# Test: 2164782
# Classes: 47

----------------------------------------start of run function 
 Nvidia-smi: 0.3560791015625 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

labels  tensor([0, 1, 2,  ..., 8, 2, 4])
epoch  0
the output layer 
self.num_batch (get_in_degree_bucketing) 32
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  32
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.10990428924560547
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  32
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0002117156982421875
self.weights_list  [0.0324513788910877, 0.038729008856850704, 0.03304658367714464, 0.032436117229906754, 0.03594629930152464, 0.03708583666970204, 0.037523337623555864, 0.03213597122668145, 0.03212579678589415, 0.03206475014117036, 0.03198335461487198, 0.03222754119376714, 0.032278413397703626, 0.032283500618097274, 0.032120709565500505, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029083638990491986]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.00962066650390625
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11040830612182617
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  5.849976062774658
self.buckets_partition() spend  sec:  0.12004399299621582
input layer
dataloader gen time  19.172982692718506
weights_list [0.0324513788910877, 0.038729008856850704, 0.03304658367714464, 0.032436117229906754, 0.03594629930152464, 0.03708583666970204, 0.037523337623555864, 0.03213597122668145, 0.03212579678589415, 0.03206475014117036, 0.03198335461487198, 0.03222754119376714, 0.032278413397703626, 0.032283500618097274, 0.032120709565500505, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029083638990491986]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
step  20
step  21
step  22
step  23
step  24
step  25
step  26
step  27
step  28
step  29
step  30
step  31
pure train time  4.443455219268799
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 11.79052734375 GB
    Memory Allocated: 0.4062018394470215  GigaBytes
Max Memory Allocated: 9.614779949188232  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 5.198888301849365
epoch  1
the output layer 
self.num_batch (get_in_degree_bucketing) 32
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  32
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.11393523216247559
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  32
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.00048065185546875
self.weights_list  [0.0324513788910877, 0.038729008856850704, 0.03304658367714464, 0.032436117229906754, 0.03594629930152464, 0.03708583666970204, 0.037523337623555864, 0.03213597122668145, 0.03212579678589415, 0.03206475014117036, 0.03198335461487198, 0.03222754119376714, 0.032278413397703626, 0.032283500618097274, 0.032120709565500505, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029083638990491986]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.01171422004699707
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11469197273254395
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  5.766147613525391
self.buckets_partition() spend  sec:  0.12642788887023926
input layer
dataloader gen time  18.965238571166992
weights_list [0.0324513788910877, 0.038729008856850704, 0.03304658367714464, 0.032436117229906754, 0.03594629930152464, 0.03708583666970204, 0.037523337623555864, 0.03213597122668145, 0.03212579678589415, 0.03206475014117036, 0.03198335461487198, 0.03222754119376714, 0.032278413397703626, 0.032283500618097274, 0.032120709565500505, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029083638990491986]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
step  20
step  21
step  22
step  23
step  24
step  25
step  26
step  27
step  28
step  29
step  30
step  31
pure train time  3.1873247623443604
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 11.79248046875 GB
    Memory Allocated: 0.4054865837097168  GigaBytes
Max Memory Allocated: 9.63928747177124  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 3.4783828258514404
epoch  2
the output layer 
self.num_batch (get_in_degree_bucketing) 32
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  32
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.10654878616333008
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  32
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0004172325134277344
self.weights_list  [0.0324513788910877, 0.038729008856850704, 0.03304658367714464, 0.032436117229906754, 0.03594629930152464, 0.03708583666970204, 0.037523337623555864, 0.03213597122668145, 0.03212579678589415, 0.03206475014117036, 0.03198335461487198, 0.03222754119376714, 0.032278413397703626, 0.032283500618097274, 0.032120709565500505, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029083638990491986]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.011565446853637695
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.10738921165466309
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  5.874643564224243
self.buckets_partition() spend  sec:  0.11897706985473633
input layer
dataloader gen time  18.453343152999878
weights_list [0.0324513788910877, 0.038729008856850704, 0.03304658367714464, 0.032436117229906754, 0.03594629930152464, 0.03708583666970204, 0.037523337623555864, 0.03213597122668145, 0.03212579678589415, 0.03206475014117036, 0.03198335461487198, 0.03222754119376714, 0.032278413397703626, 0.032283500618097274, 0.032120709565500505, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029083638990491986]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
step  20
step  21
step  22
step  23
step  24
step  25
step  26
step  27
step  28
step  29
step  30
step  31
pure train time  3.1244170665740967
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 11.79833984375 GB
    Memory Allocated: 0.40762948989868164  GigaBytes
Max Memory Allocated: 9.63928747177124  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 2.7321722507476807
epoch  3
load pickle file time  0.5661580562591553
the output layer 
self.num_batch (get_in_degree_bucketing) 32
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  32
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.1171274185180664
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  32
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.00041794776916503906
self.weights_list  [0.0324513788910877, 0.038729008856850704, 0.03304658367714464, 0.032436117229906754, 0.03594629930152464, 0.03708583666970204, 0.037523337623555864, 0.03213597122668145, 0.03212579678589415, 0.03206475014117036, 0.03198335461487198, 0.03222754119376714, 0.032278413397703626, 0.032283500618097274, 0.032120709565500505, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029083638990491986]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.011640548706054688
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11780476570129395
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  5.837799310684204
self.buckets_partition() spend  sec:  0.12946772575378418
input layer
dataloader gen time  18.382703065872192
weights_list [0.0324513788910877, 0.038729008856850704, 0.03304658367714464, 0.032436117229906754, 0.03594629930152464, 0.03708583666970204, 0.037523337623555864, 0.03213597122668145, 0.03212579678589415, 0.03206475014117036, 0.03198335461487198, 0.03222754119376714, 0.032278413397703626, 0.032283500618097274, 0.032120709565500505, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029083638990491986]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
step  20
step  21
step  22
step  23
step  24
step  25
step  26
step  27
step  28
step  29
step  30
step  31
pure train time  3.3549599647521973
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 11.79833984375 GB
    Memory Allocated: 0.4066305160522461  GigaBytes
Max Memory Allocated: 9.63928747177124  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 2.2333765029907227
epoch  4
load pickle file time  0.6436138153076172
the output layer 
self.num_batch (get_in_degree_bucketing) 32
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  32
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.4329376220703125
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  32
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0008313655853271484
self.weights_list  [0.0324513788910877, 0.038729008856850704, 0.03304658367714464, 0.032436117229906754, 0.03594629930152464, 0.03708583666970204, 0.037523337623555864, 0.03213597122668145, 0.03212579678589415, 0.03206475014117036, 0.03198335461487198, 0.03222754119376714, 0.032278413397703626, 0.032283500618097274, 0.032120709565500505, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029083638990491986]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.01503610610961914
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.4341239929199219
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  6.185110330581665
self.buckets_partition() spend  sec:  0.4492034912109375
input layer
dataloader gen time  21.49978280067444
weights_list [0.0324513788910877, 0.038729008856850704, 0.03304658367714464, 0.032436117229906754, 0.03594629930152464, 0.03708583666970204, 0.037523337623555864, 0.03213597122668145, 0.03212579678589415, 0.03206475014117036, 0.03198335461487198, 0.03222754119376714, 0.032278413397703626, 0.032283500618097274, 0.032120709565500505, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029083638990491986]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
step  20
step  21
step  22
step  23
step  24
step  25
step  26
step  27
step  28
step  29
step  30
step  31
pure train time  3.320115327835083
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 11.79833984375 GB
    Memory Allocated: 0.4067215919494629  GigaBytes
Max Memory Allocated: 9.63928747177124  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7761473655700684
epoch  5
load pickle file time  0.5319395065307617
the output layer 
self.num_batch (get_in_degree_bucketing) 32
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  32
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.11844134330749512
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  32
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0005257129669189453
self.weights_list  [0.0324513788910877, 0.038729008856850704, 0.03304658367714464, 0.032436117229906754, 0.03594629930152464, 0.03708583666970204, 0.037523337623555864, 0.03213597122668145, 0.03212579678589415, 0.03206475014117036, 0.03198335461487198, 0.03222754119376714, 0.032278413397703626, 0.032283500618097274, 0.032120709565500505, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029083638990491986]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.011396408081054688
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.1192162036895752
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  5.80454158782959
self.buckets_partition() spend  sec:  0.13063955307006836
input layer
dataloader gen time  20.73766589164734
weights_list [0.0324513788910877, 0.038729008856850704, 0.03304658367714464, 0.032436117229906754, 0.03594629930152464, 0.03708583666970204, 0.037523337623555864, 0.03213597122668145, 0.03212579678589415, 0.03206475014117036, 0.03198335461487198, 0.03222754119376714, 0.032278413397703626, 0.032283500618097274, 0.032120709565500505, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029083638990491986]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
step  20
step  21
step  22
step  23
step  24
step  25
step  26
step  27
step  28
step  29
step  30
step  31
pure train time  3.261270523071289
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 11.79833984375 GB
    Memory Allocated: 0.40843915939331055  GigaBytes
Max Memory Allocated: 9.63928747177124  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5500069856643677
epoch  6
load pickle file time  0.5784463882446289
the output layer 
self.num_batch (get_in_degree_bucketing) 32
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  32
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.11396408081054688
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  32
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.00044417381286621094
self.weights_list  [0.0324513788910877, 0.038729008856850704, 0.03304658367714464, 0.032436117229906754, 0.03594629930152464, 0.03708583666970204, 0.037523337623555864, 0.03213597122668145, 0.03212579678589415, 0.03206475014117036, 0.03198335461487198, 0.03222754119376714, 0.032278413397703626, 0.032283500618097274, 0.032120709565500505, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029083638990491986]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.012357711791992188
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11468315124511719
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  5.801701545715332
self.buckets_partition() spend  sec:  0.1270606517791748
input layer
dataloader gen time  19.98633885383606
weights_list [0.0324513788910877, 0.038729008856850704, 0.03304658367714464, 0.032436117229906754, 0.03594629930152464, 0.03708583666970204, 0.037523337623555864, 0.03213597122668145, 0.03212579678589415, 0.03206475014117036, 0.03198335461487198, 0.03222754119376714, 0.032278413397703626, 0.032283500618097274, 0.032120709565500505, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029083638990491986]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
step  20
step  21
step  22
step  23
step  24
step  25
step  26
step  27
step  28
step  29
step  30
step  31
pure train time  3.1886017322540283
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 11.79833984375 GB
    Memory Allocated: 0.40618038177490234  GigaBytes
Max Memory Allocated: 9.63928747177124  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.3939158916473389
epoch  7
load pickle file time  0.5694413185119629
the output layer 
self.num_batch (get_in_degree_bucketing) 32
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  32
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.1144871711730957
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  32
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.00041484832763671875
self.weights_list  [0.0324513788910877, 0.038729008856850704, 0.03304658367714464, 0.032436117229906754, 0.03594629930152464, 0.03708583666970204, 0.037523337623555864, 0.03213597122668145, 0.03212579678589415, 0.03206475014117036, 0.03198335461487198, 0.03222754119376714, 0.032278413397703626, 0.032283500618097274, 0.032120709565500505, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029083638990491986]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.01318502426147461
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11511516571044922
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  5.908367156982422
self.buckets_partition() spend  sec:  0.12832331657409668
input layer
dataloader gen time  19.310263872146606
weights_list [0.0324513788910877, 0.038729008856850704, 0.03304658367714464, 0.032436117229906754, 0.03594629930152464, 0.03708583666970204, 0.037523337623555864, 0.03213597122668145, 0.03212579678589415, 0.03206475014117036, 0.03198335461487198, 0.03222754119376714, 0.032278413397703626, 0.032283500618097274, 0.032120709565500505, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029083638990491986]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
step  20
step  21
step  22
step  23
step  24
step  25
step  26
step  27
step  28
step  29
step  30
step  31
pure train time  3.209796190261841
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 11.79833984375 GB
    Memory Allocated: 0.4063401222229004  GigaBytes
Max Memory Allocated: 9.63928747177124  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.2740042209625244
epoch  8
load pickle file time  0.5520346164703369
the output layer 
self.num_batch (get_in_degree_bucketing) 32
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  32
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.12978458404541016
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  32
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0003154277801513672
self.weights_list  [0.0324513788910877, 0.038729008856850704, 0.03304658367714464, 0.032436117229906754, 0.03594629930152464, 0.03708583666970204, 0.037523337623555864, 0.03213597122668145, 0.03212579678589415, 0.03206475014117036, 0.03198335461487198, 0.03222754119376714, 0.032278413397703626, 0.032283500618097274, 0.032120709565500505, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029083638990491986]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.010747194290161133
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.13037657737731934
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  5.935323715209961
self.buckets_partition() spend  sec:  0.141143798828125
input layer
dataloader gen time  18.00364398956299
weights_list [0.0324513788910877, 0.038729008856850704, 0.03304658367714464, 0.032436117229906754, 0.03594629930152464, 0.03708583666970204, 0.037523337623555864, 0.03213597122668145, 0.03212579678589415, 0.03206475014117036, 0.03198335461487198, 0.03222754119376714, 0.032278413397703626, 0.032283500618097274, 0.032120709565500505, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029083638990491986]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
step  20
step  21
step  22
step  23
step  24
step  25
step  26
step  27
step  28
step  29
step  30
step  31
pure train time  3.23110294342041
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 11.79833984375 GB
    Memory Allocated: 0.40617799758911133  GigaBytes
Max Memory Allocated: 9.63928747177124  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.1739524602890015
epoch  9
load pickle file time  0.5287318229675293
the output layer 
self.num_batch (get_in_degree_bucketing) 32
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  32
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.10504961013793945
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  32
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0004146099090576172
self.weights_list  [0.0324513788910877, 0.038729008856850704, 0.03304658367714464, 0.032436117229906754, 0.03594629930152464, 0.03708583666970204, 0.037523337623555864, 0.03213597122668145, 0.03212579678589415, 0.03206475014117036, 0.03198335461487198, 0.03222754119376714, 0.032278413397703626, 0.032283500618097274, 0.032120709565500505, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029083638990491986]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.01156473159790039
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.10570406913757324
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  5.827606439590454
self.buckets_partition() spend  sec:  0.11729192733764648
input layer
dataloader gen time  17.49758243560791
weights_list [0.0324513788910877, 0.038729008856850704, 0.03304658367714464, 0.032436117229906754, 0.03594629930152464, 0.03708583666970204, 0.037523337623555864, 0.03213597122668145, 0.03212579678589415, 0.03206475014117036, 0.03198335461487198, 0.03222754119376714, 0.032278413397703626, 0.032283500618097274, 0.032120709565500505, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029083638990491986]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
step  20
step  21
step  22
step  23
step  24
step  25
step  26
step  27
step  28
step  29
step  30
step  31
pure train time  3.183089256286621
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 11.79833984375 GB
    Memory Allocated: 0.4067397117614746  GigaBytes
Max Memory Allocated: 9.63928747177124  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.0808231830596924
epoch  10
load pickle file time  0.5763030052185059
the output layer 
self.num_batch (get_in_degree_bucketing) 32
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  32
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.11401629447937012
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  32
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.00035071372985839844
self.weights_list  [0.0324513788910877, 0.038729008856850704, 0.03304658367714464, 0.032436117229906754, 0.03594629930152464, 0.03708583666970204, 0.037523337623555864, 0.03213597122668145, 0.03212579678589415, 0.03206475014117036, 0.03198335461487198, 0.03222754119376714, 0.032278413397703626, 0.032283500618097274, 0.032120709565500505, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029083638990491986]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.012319087982177734
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11462116241455078
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  5.8330771923065186
self.buckets_partition() spend  sec:  0.12695884704589844
input layer
dataloader gen time  17.7734534740448
weights_list [0.0324513788910877, 0.038729008856850704, 0.03304658367714464, 0.032436117229906754, 0.03594629930152464, 0.03708583666970204, 0.037523337623555864, 0.03213597122668145, 0.03212579678589415, 0.03206475014117036, 0.03198335461487198, 0.03222754119376714, 0.032278413397703626, 0.032283500618097274, 0.032120709565500505, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029083638990491986]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
step  20
step  21
step  22
step  23
step  24
step  25
step  26
step  27
step  28
step  29
step  30
step  31
pure train time  3.133776903152466
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 11.79833984375 GB
    Memory Allocated: 0.40552520751953125  GigaBytes
Max Memory Allocated: 9.63928747177124  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.9974876642227173
epoch  11
load pickle file time  0.5511071681976318
the output layer 
self.num_batch (get_in_degree_bucketing) 32
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  32
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.11398983001708984
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  32
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.00029587745666503906
self.weights_list  [0.0324513788910877, 0.038729008856850704, 0.03304658367714464, 0.032436117229906754, 0.03594629930152464, 0.03708583666970204, 0.037523337623555864, 0.03213597122668145, 0.03212579678589415, 0.03206475014117036, 0.03198335461487198, 0.03222754119376714, 0.032278413397703626, 0.032283500618097274, 0.032120709565500505, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029083638990491986]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.012520313262939453
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11446070671081543
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  5.820204019546509
self.buckets_partition() spend  sec:  0.1269989013671875
input layer
dataloader gen time  18.13842272758484
weights_list [0.0324513788910877, 0.038729008856850704, 0.03304658367714464, 0.032436117229906754, 0.03594629930152464, 0.03708583666970204, 0.037523337623555864, 0.03213597122668145, 0.03212579678589415, 0.03206475014117036, 0.03198335461487198, 0.03222754119376714, 0.032278413397703626, 0.032283500618097274, 0.032120709565500505, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029083638990491986]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
step  20
step  21
step  22
step  23
step  24
step  25
step  26
step  27
step  28
step  29
step  30
step  31
pure train time  3.20198130607605
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 11.79833984375 GB
    Memory Allocated: 0.4066190719604492  GigaBytes
Max Memory Allocated: 9.63928747177124  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.9551994800567627
epoch  12
load pickle file time  0.5630347728729248
the output layer 
self.num_batch (get_in_degree_bucketing) 32
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  32
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.10498285293579102
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  32
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.00028514862060546875
self.weights_list  [0.0324513788910877, 0.038729008856850704, 0.03304658367714464, 0.032436117229906754, 0.03594629930152464, 0.03708583666970204, 0.037523337623555864, 0.03213597122668145, 0.03212579678589415, 0.03206475014117036, 0.03198335461487198, 0.03222754119376714, 0.032278413397703626, 0.032283500618097274, 0.032120709565500505, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029083638990491986]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.01085209846496582
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.10558557510375977
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  5.825940132141113
self.buckets_partition() spend  sec:  0.11645984649658203
input layer
dataloader gen time  17.368492603302002
weights_list [0.0324513788910877, 0.038729008856850704, 0.03304658367714464, 0.032436117229906754, 0.03594629930152464, 0.03708583666970204, 0.037523337623555864, 0.03213597122668145, 0.03212579678589415, 0.03206475014117036, 0.03198335461487198, 0.03222754119376714, 0.032278413397703626, 0.032283500618097274, 0.032120709565500505, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029083638990491986]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
step  20
step  21
step  22
step  23
step  24
step  25
step  26
step  27
step  28
step  29
step  30
step  31
pure train time  3.168278455734253
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 11.79833984375 GB
    Memory Allocated: 0.40764904022216797  GigaBytes
Max Memory Allocated: 9.63928747177124  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.9103860259056091
epoch  13
load pickle file time  0.5022187232971191
the output layer 
self.num_batch (get_in_degree_bucketing) 32
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  32
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.11461043357849121
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  32
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0003998279571533203
self.weights_list  [0.0324513788910877, 0.038729008856850704, 0.03304658367714464, 0.032436117229906754, 0.03594629930152464, 0.03708583666970204, 0.037523337623555864, 0.03213597122668145, 0.03212579678589415, 0.03206475014117036, 0.03198335461487198, 0.03222754119376714, 0.032278413397703626, 0.032283500618097274, 0.032120709565500505, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029083638990491986]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.01046133041381836
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11524653434753418
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  5.839717149734497
self.buckets_partition() spend  sec:  0.12572669982910156
input layer
dataloader gen time  17.474456310272217
weights_list [0.0324513788910877, 0.038729008856850704, 0.03304658367714464, 0.032436117229906754, 0.03594629930152464, 0.03708583666970204, 0.037523337623555864, 0.03213597122668145, 0.03212579678589415, 0.03206475014117036, 0.03198335461487198, 0.03222754119376714, 0.032278413397703626, 0.032283500618097274, 0.032120709565500505, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029083638990491986]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
step  20
step  21
step  22
step  23
step  24
step  25
step  26
step  27
step  28
step  29
step  30
step  31
pure train time  3.2292706966400146
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 11.79833984375 GB
    Memory Allocated: 0.40628480911254883  GigaBytes
Max Memory Allocated: 9.63928747177124  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.8735139966011047
epoch  14
load pickle file time  0.5600361824035645
the output layer 
self.num_batch (get_in_degree_bucketing) 32
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  32
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.1142113208770752
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  32
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0003173351287841797
self.weights_list  [0.0324513788910877, 0.038729008856850704, 0.03304658367714464, 0.032436117229906754, 0.03594629930152464, 0.03708583666970204, 0.037523337623555864, 0.03213597122668145, 0.03212579678589415, 0.03206475014117036, 0.03198335461487198, 0.03222754119376714, 0.032278413397703626, 0.032283500618097274, 0.032120709565500505, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029083638990491986]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.010110139846801758
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11479878425598145
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  5.780425071716309
self.buckets_partition() spend  sec:  0.12492847442626953
input layer
dataloader gen time  17.354270696640015
weights_list [0.0324513788910877, 0.038729008856850704, 0.03304658367714464, 0.032436117229906754, 0.03594629930152464, 0.03708583666970204, 0.037523337623555864, 0.03213597122668145, 0.03212579678589415, 0.03206475014117036, 0.03198335461487198, 0.03222754119376714, 0.032278413397703626, 0.032283500618097274, 0.032120709565500505, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029083638990491986]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
step  20
step  21
step  22
step  23
step  24
step  25
step  26
step  27
step  28
step  29
step  30
step  31
pure train time  3.26932692527771
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 11.79833984375 GB
    Memory Allocated: 0.40575504302978516  GigaBytes
Max Memory Allocated: 9.63928747177124  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.8340076804161072
epoch  15
load pickle file time  0.5941746234893799
the output layer 
self.num_batch (get_in_degree_bucketing) 32
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  32
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.10570526123046875
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  32
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.00037860870361328125
self.weights_list  [0.0324513788910877, 0.038729008856850704, 0.03304658367714464, 0.032436117229906754, 0.03594629930152464, 0.03708583666970204, 0.037523337623555864, 0.03213597122668145, 0.03212579678589415, 0.03206475014117036, 0.03198335461487198, 0.03222754119376714, 0.032278413397703626, 0.032283500618097274, 0.032120709565500505, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029083638990491986]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.010567188262939453
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.10631608963012695
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  5.832400798797607
self.buckets_partition() spend  sec:  0.11690139770507812
input layer
dataloader gen time  17.551273345947266
weights_list [0.0324513788910877, 0.038729008856850704, 0.03304658367714464, 0.032436117229906754, 0.03594629930152464, 0.03708583666970204, 0.037523337623555864, 0.03213597122668145, 0.03212579678589415, 0.03206475014117036, 0.03198335461487198, 0.03222754119376714, 0.032278413397703626, 0.032283500618097274, 0.032120709565500505, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029083638990491986]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
step  20
step  21
step  22
step  23
step  24
step  25
step  26
step  27
step  28
step  29
step  30
step  31
pure train time  3.2516748905181885
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 11.79833984375 GB
    Memory Allocated: 0.405911922454834  GigaBytes
Max Memory Allocated: 9.639608383178711  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.8007311820983887
epoch  16
load pickle file time  0.5518600940704346
the output layer 
self.num_batch (get_in_degree_bucketing) 32
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  32
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.11561417579650879
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  32
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0002918243408203125
self.weights_list  [0.0324513788910877, 0.038729008856850704, 0.03304658367714464, 0.032436117229906754, 0.03594629930152464, 0.03708583666970204, 0.037523337623555864, 0.03213597122668145, 0.03212579678589415, 0.03206475014117036, 0.03198335461487198, 0.03222754119376714, 0.032278413397703626, 0.032283500618097274, 0.032120709565500505, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029083638990491986]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.010990142822265625
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11618208885192871
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  5.846198797225952
self.buckets_partition() spend  sec:  0.12719202041625977
input layer
dataloader gen time  17.518430948257446
weights_list [0.0324513788910877, 0.038729008856850704, 0.03304658367714464, 0.032436117229906754, 0.03594629930152464, 0.03708583666970204, 0.037523337623555864, 0.03213597122668145, 0.03212579678589415, 0.03206475014117036, 0.03198335461487198, 0.03222754119376714, 0.032278413397703626, 0.032283500618097274, 0.032120709565500505, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029083638990491986]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
step  20
step  21
step  22
step  23
step  24
step  25
step  26
step  27
step  28
step  29
step  30
step  31
pure train time  3.257868766784668
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 11.79833984375 GB
    Memory Allocated: 0.40641355514526367  GigaBytes
Max Memory Allocated: 9.639608383178711  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.7623776197433472
epoch  17
load pickle file time  0.5326521396636963
the output layer 
self.num_batch (get_in_degree_bucketing) 32
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  32
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.11357378959655762
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  32
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0003390312194824219
self.weights_list  [0.0324513788910877, 0.038729008856850704, 0.03304658367714464, 0.032436117229906754, 0.03594629930152464, 0.03708583666970204, 0.037523337623555864, 0.03213597122668145, 0.03212579678589415, 0.03206475014117036, 0.03198335461487198, 0.03222754119376714, 0.032278413397703626, 0.032283500618097274, 0.032120709565500505, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029083638990491986]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.010090351104736328
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.1141347885131836
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  5.837872743606567
self.buckets_partition() spend  sec:  0.12424397468566895
input layer
dataloader gen time  17.79844617843628
weights_list [0.0324513788910877, 0.038729008856850704, 0.03304658367714464, 0.032436117229906754, 0.03594629930152464, 0.03708583666970204, 0.037523337623555864, 0.03213597122668145, 0.03212579678589415, 0.03206475014117036, 0.03198335461487198, 0.03222754119376714, 0.032278413397703626, 0.032283500618097274, 0.032120709565500505, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029083638990491986]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
step  20
step  21
step  22
step  23
step  24
step  25
step  26
step  27
step  28
step  29
step  30
step  31
pure train time  3.1983139514923096
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 11.79833984375 GB
    Memory Allocated: 0.40651750564575195  GigaBytes
Max Memory Allocated: 9.639608383178711  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.7314296960830688
epoch  18
load pickle file time  0.5067145824432373
the output layer 
self.num_batch (get_in_degree_bucketing) 32
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  32
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.10522890090942383
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  32
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0003006458282470703
self.weights_list  [0.0324513788910877, 0.038729008856850704, 0.03304658367714464, 0.032436117229906754, 0.03594629930152464, 0.03708583666970204, 0.037523337623555864, 0.03213597122668145, 0.03212579678589415, 0.03206475014117036, 0.03198335461487198, 0.03222754119376714, 0.032278413397703626, 0.032283500618097274, 0.032120709565500505, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029083638990491986]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.010384082794189453
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.10578203201293945
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  5.8238866329193115
self.buckets_partition() spend  sec:  0.11618471145629883
input layer
dataloader gen time  17.527137517929077
weights_list [0.0324513788910877, 0.038729008856850704, 0.03304658367714464, 0.032436117229906754, 0.03594629930152464, 0.03708583666970204, 0.037523337623555864, 0.03213597122668145, 0.03212579678589415, 0.03206475014117036, 0.03198335461487198, 0.03222754119376714, 0.032278413397703626, 0.032283500618097274, 0.032120709565500505, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029083638990491986]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
step  20
step  21
step  22
step  23
step  24
step  25
step  26
step  27
step  28
step  29
step  30
step  31
pure train time  3.2440454959869385
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 11.79833984375 GB
    Memory Allocated: 0.4062662124633789  GigaBytes
Max Memory Allocated: 9.639608383178711  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.7001489400863647
epoch  19
load pickle file time  0.5119123458862305
the output layer 
self.num_batch (get_in_degree_bucketing) 32
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  32
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.296236515045166
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  32
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0003380775451660156
self.weights_list  [0.0324513788910877, 0.038729008856850704, 0.03304658367714464, 0.032436117229906754, 0.03594629930152464, 0.03708583666970204, 0.037523337623555864, 0.03213597122668145, 0.03212579678589415, 0.03206475014117036, 0.03198335461487198, 0.03222754119376714, 0.032278413397703626, 0.032283500618097274, 0.032120709565500505, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029083638990491986]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.011156797409057617
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.2968149185180664
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  6.01885724067688
self.buckets_partition() spend  sec:  0.3080015182495117
input layer
dataloader gen time  17.53044819831848
weights_list [0.0324513788910877, 0.038729008856850704, 0.03304658367714464, 0.032436117229906754, 0.03594629930152464, 0.03708583666970204, 0.037523337623555864, 0.03213597122668145, 0.03212579678589415, 0.03206475014117036, 0.03198335461487198, 0.03222754119376714, 0.032278413397703626, 0.032283500618097274, 0.032120709565500505, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029154860076003073, 0.029083638990491986]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
step  20
step  21
step  22
step  23
step  24
step  25
step  26
step  27
step  28
step  29
step  30
step  31
pure train time  3.295382499694824
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 11.79833984375 GB
    Memory Allocated: 0.406461238861084  GigaBytes
Max Memory Allocated: 9.639608383178711  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.6722081899642944
Total (block generation + training)time/epoch 25.71870269494898
pure train time/epoch 3.2277434915304184
dataloader time  [18.382703065872192, 21.49978280067444, 20.73766589164734, 19.98633885383606, 19.310263872146606, 18.00364398956299, 17.49758243560791, 17.7734534740448, 18.13842272758484, 17.368492603302002, 17.474456310272217, 17.354270696640015, 17.551273345947266, 17.518430948257446, 17.79844617843628, 17.527137517929077, 17.53044819831848]
dataloader time avg per epoch 17.7574094075423

num_input_list  [21318852, 21325539, 21313553, 21325507, 21327141, 21323189, 21326077, 21314611, 21313256, 21319778, 21313128, 21322684, 21319689, 21305315, 21315765, 21303424, 21319272, 21311626, 21317910, 21312955]
