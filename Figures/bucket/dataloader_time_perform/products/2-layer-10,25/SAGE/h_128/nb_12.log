main start at this time 1712911805.8037062
-----------------------------------------before load data 
 Nvidia-smi: 0.3560791015625 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

#nodes: 2449029
#edges: 123718024
#classes: 47
success----------------------------------------
196571
39255
2164782
# Nodes: 2400608
# Edges: 123718024
# Train: 196571
# Val: 39255
# Test: 2164782
# Classes: 47

----------------------------------------start of run function 
 Nvidia-smi: 0.3560791015625 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

labels  tensor([0, 1, 2,  ..., 8, 2, 4])
epoch  0
the output layer 
self.num_batch (get_in_degree_bucketing) 12
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  12
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1464
G_BUCKET_ID_list [[11, 3, 2], [19, 13, 0], [23, 8], [14, 9, 7], [17, 10, 4], [6, 5, 1], [16, 12], [22], [21], [20], [18], [15]]
G_BUCKET_ID_list length 12
backpack scheduling spend  0.12091660499572754
len(g_bucket_nids_list)  12
len(local_split_batches_nid_list)  12
current group_mem  1.403347350375729
current group_mem  3.001967961271789
current group_mem  3.0000069573568977
current group_mem  3.0006915325832333
current group_mem  2.9975327611562568
current group_mem  1.0749997896799088
current group_mem  2.939728300182595
current group_mem  2.232189497508864
current group_mem  2.1946883605045633
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.4645761989081698
batches output list generation spend  0.0002987384796142578
self.weights_list  [0.08493114447197196, 0.08506849942260049, 0.08395439815639133, 0.0870270792741554, 0.08625382177432073, 0.0849972783370894, 0.08370003713670887, 0.08103942087083038, 0.08075453652878603, 0.08072401320642414, 0.08071383876563684, 0.08083593205508442]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.010398149490356445
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.12147855758666992
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  2.262770652770996
self.buckets_partition() spend  sec:  0.1318974494934082
input layer
dataloader gen time  14.561094760894775
weights_list [0.08493114447197196, 0.08506849942260049, 0.08395439815639133, 0.0870270792741554, 0.08625382177432073, 0.0849972783370894, 0.08370003713670887, 0.08103942087083038, 0.08075453652878603, 0.08072401320642414, 0.08071383876563684, 0.08083593205508442]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
pure train time  5.687086343765259
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 21.90771484375 GB
    Memory Allocated: 0.5922765731811523  GigaBytes
Max Memory Allocated: 21.219299793243408  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 5.205336570739746
epoch  1
the output layer 
self.num_batch (get_in_degree_bucketing) 12
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  12
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1464
G_BUCKET_ID_list [[11, 3, 2], [19, 13, 0], [23, 8], [14, 9, 7], [17, 10, 4], [6, 5, 1], [16, 12], [22], [21], [20], [18], [15]]
G_BUCKET_ID_list length 12
backpack scheduling spend  0.13234806060791016
len(g_bucket_nids_list)  12
len(local_split_batches_nid_list)  12
current group_mem  1.403347350375729
current group_mem  3.001967961271789
current group_mem  3.0000069573568977
current group_mem  3.0006915325832333
current group_mem  2.9975327611562568
current group_mem  1.0749997896799088
current group_mem  2.939728300182595
current group_mem  2.232189497508864
current group_mem  2.1946883605045633
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.4645761989081698
batches output list generation spend  0.00043463706970214844
self.weights_list  [0.08493114447197196, 0.08506849942260049, 0.08395439815639133, 0.0870270792741554, 0.08625382177432073, 0.0849972783370894, 0.08370003713670887, 0.08103942087083038, 0.08075453652878603, 0.08072401320642414, 0.08071383876563684, 0.08083593205508442]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.012681007385253906
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.1329793930053711
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  2.291015625
self.buckets_partition() spend  sec:  0.14568233489990234
input layer
dataloader gen time  14.669384241104126
weights_list [0.08493114447197196, 0.08506849942260049, 0.08395439815639133, 0.0870270792741554, 0.08625382177432073, 0.0849972783370894, 0.08370003713670887, 0.08103942087083038, 0.08075453652878603, 0.08072401320642414, 0.08071383876563684, 0.08083593205508442]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
pure train time  2.250253677368164
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 21.86669921875 GB
    Memory Allocated: 0.5930080413818359  GigaBytes
Max Memory Allocated: 21.220460891723633  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 3.4788286685943604
epoch  2
the output layer 
self.num_batch (get_in_degree_bucketing) 12
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  12
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1464
G_BUCKET_ID_list [[11, 3, 2], [19, 13, 0], [23, 8], [14, 9, 7], [17, 10, 4], [6, 5, 1], [16, 12], [22], [21], [20], [18], [15]]
G_BUCKET_ID_list length 12
backpack scheduling spend  0.12270975112915039
len(g_bucket_nids_list)  12
len(local_split_batches_nid_list)  12
current group_mem  1.403347350375729
current group_mem  3.001967961271789
current group_mem  3.0000069573568977
current group_mem  3.0006915325832333
current group_mem  2.9975327611562568
current group_mem  1.0749997896799088
current group_mem  2.939728300182595
current group_mem  2.232189497508864
current group_mem  2.1946883605045633
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.4645761989081698
batches output list generation spend  0.0010652542114257812
self.weights_list  [0.08493114447197196, 0.08506849942260049, 0.08395439815639133, 0.0870270792741554, 0.08625382177432073, 0.0849972783370894, 0.08370003713670887, 0.08103942087083038, 0.08075453652878603, 0.08072401320642414, 0.08071383876563684, 0.08083593205508442]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.011343002319335938
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.12395596504211426
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  2.2452452182769775
self.buckets_partition() spend  sec:  0.13532066345214844
input layer
dataloader gen time  13.352954387664795
weights_list [0.08493114447197196, 0.08506849942260049, 0.08395439815639133, 0.0870270792741554, 0.08625382177432073, 0.0849972783370894, 0.08370003713670887, 0.08103942087083038, 0.08075453652878603, 0.08072401320642414, 0.08071383876563684, 0.08083593205508442]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
pure train time  2.1655068397521973
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 21.86669921875 GB
    Memory Allocated: 0.5938916206359863  GigaBytes
Max Memory Allocated: 21.220460891723633  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 2.7352383136749268
epoch  3
load pickle file time  0.4626595973968506
the output layer 
self.num_batch (get_in_degree_bucketing) 12
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  12
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1464
G_BUCKET_ID_list [[11, 3, 2], [19, 13, 0], [23, 8], [14, 9, 7], [17, 10, 4], [6, 5, 1], [16, 12], [22], [21], [20], [18], [15]]
G_BUCKET_ID_list length 12
backpack scheduling spend  0.1227562427520752
len(g_bucket_nids_list)  12
len(local_split_batches_nid_list)  12
current group_mem  1.403347350375729
current group_mem  3.001967961271789
current group_mem  3.0000069573568977
current group_mem  3.0006915325832333
current group_mem  2.9975327611562568
current group_mem  1.0749997896799088
current group_mem  2.939728300182595
current group_mem  2.232189497508864
current group_mem  2.1946883605045633
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.4645761989081698
batches output list generation spend  0.0005469322204589844
self.weights_list  [0.08493114447197196, 0.08506849942260049, 0.08395439815639133, 0.0870270792741554, 0.08625382177432073, 0.0849972783370894, 0.08370003713670887, 0.08103942087083038, 0.08075453652878603, 0.08072401320642414, 0.08071383876563684, 0.08083593205508442]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.009693384170532227
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.12342596054077148
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  2.265634298324585
self.buckets_partition() spend  sec:  0.13313674926757812
input layer
dataloader gen time  13.357699871063232
weights_list [0.08493114447197196, 0.08506849942260049, 0.08395439815639133, 0.0870270792741554, 0.08625382177432073, 0.0849972783370894, 0.08370003713670887, 0.08103942087083038, 0.08075453652878603, 0.08072401320642414, 0.08071383876563684, 0.08083593205508442]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
pure train time  2.1924757957458496
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 22.98779296875 GB
    Memory Allocated: 0.5918002128601074  GigaBytes
Max Memory Allocated: 21.23098850250244  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 2.2376656532287598
epoch  4
load pickle file time  0.37244510650634766
the output layer 
self.num_batch (get_in_degree_bucketing) 12
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  12
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1464
G_BUCKET_ID_list [[11, 3, 2], [19, 13, 0], [23, 8], [14, 9, 7], [17, 10, 4], [6, 5, 1], [16, 12], [22], [21], [20], [18], [15]]
G_BUCKET_ID_list length 12
backpack scheduling spend  0.12291598320007324
len(g_bucket_nids_list)  12
len(local_split_batches_nid_list)  12
current group_mem  1.403347350375729
current group_mem  3.001967961271789
current group_mem  3.0000069573568977
current group_mem  3.0006915325832333
current group_mem  2.9975327611562568
current group_mem  1.0749997896799088
current group_mem  2.939728300182595
current group_mem  2.232189497508864
current group_mem  2.1946883605045633
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.4645761989081698
batches output list generation spend  0.0006506443023681641
self.weights_list  [0.08493114447197196, 0.08506849942260049, 0.08395439815639133, 0.0870270792741554, 0.08625382177432073, 0.0849972783370894, 0.08370003713670887, 0.08103942087083038, 0.08075453652878603, 0.08072401320642414, 0.08071383876563684, 0.08083593205508442]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.010531902313232422
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.12371325492858887
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  2.2571287155151367
self.buckets_partition() spend  sec:  0.13426685333251953
input layer
dataloader gen time  12.74220585823059
weights_list [0.08493114447197196, 0.08506849942260049, 0.08395439815639133, 0.0870270792741554, 0.08625382177432073, 0.0849972783370894, 0.08370003713670887, 0.08103942087083038, 0.08075453652878603, 0.08072401320642414, 0.08071383876563684, 0.08083593205508442]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
pure train time  2.1653356552124023
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 22.98779296875 GB
    Memory Allocated: 0.5919146537780762  GigaBytes
Max Memory Allocated: 21.26040267944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7773029804229736
epoch  5
load pickle file time  0.3931858539581299
the output layer 
self.num_batch (get_in_degree_bucketing) 12
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  12
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1464
G_BUCKET_ID_list [[11, 3, 2], [19, 13, 0], [23, 8], [14, 9, 7], [17, 10, 4], [6, 5, 1], [16, 12], [22], [21], [20], [18], [15]]
G_BUCKET_ID_list length 12
backpack scheduling spend  0.12272143363952637
len(g_bucket_nids_list)  12
len(local_split_batches_nid_list)  12
current group_mem  1.403347350375729
current group_mem  3.001967961271789
current group_mem  3.0000069573568977
current group_mem  3.0006915325832333
current group_mem  2.9975327611562568
current group_mem  1.0749997896799088
current group_mem  2.939728300182595
current group_mem  2.232189497508864
current group_mem  2.1946883605045633
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.4645761989081698
batches output list generation spend  0.0004980564117431641
self.weights_list  [0.08493114447197196, 0.08506849942260049, 0.08395439815639133, 0.0870270792741554, 0.08625382177432073, 0.0849972783370894, 0.08370003713670887, 0.08103942087083038, 0.08075453652878603, 0.08072401320642414, 0.08071383876563684, 0.08083593205508442]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.008957147598266602
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.12335085868835449
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  2.244025468826294
self.buckets_partition() spend  sec:  0.13232779502868652
input layer
dataloader gen time  13.20257043838501
weights_list [0.08493114447197196, 0.08506849942260049, 0.08395439815639133, 0.0870270792741554, 0.08625382177432073, 0.0849972783370894, 0.08370003713670887, 0.08103942087083038, 0.08075453652878603, 0.08072401320642414, 0.08071383876563684, 0.08083593205508442]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
pure train time  2.1764791011810303
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 22.98779296875 GB
    Memory Allocated: 0.5956149101257324  GigaBytes
Max Memory Allocated: 21.26040267944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5502287149429321
epoch  6
load pickle file time  0.4168579578399658
the output layer 
self.num_batch (get_in_degree_bucketing) 12
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  12
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1464
G_BUCKET_ID_list [[11, 3, 2], [19, 13, 0], [23, 8], [14, 9, 7], [17, 10, 4], [6, 5, 1], [16, 12], [22], [21], [20], [18], [15]]
G_BUCKET_ID_list length 12
backpack scheduling spend  0.12281107902526855
len(g_bucket_nids_list)  12
len(local_split_batches_nid_list)  12
current group_mem  1.403347350375729
current group_mem  3.001967961271789
current group_mem  3.0000069573568977
current group_mem  3.0006915325832333
current group_mem  2.9975327611562568
current group_mem  1.0749997896799088
current group_mem  2.939728300182595
current group_mem  2.232189497508864
current group_mem  2.1946883605045633
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.4645761989081698
batches output list generation spend  0.0005204677581787109
self.weights_list  [0.08493114447197196, 0.08506849942260049, 0.08395439815639133, 0.0870270792741554, 0.08625382177432073, 0.0849972783370894, 0.08370003713670887, 0.08103942087083038, 0.08075453652878603, 0.08072401320642414, 0.08071383876563684, 0.08083593205508442]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.01220703125
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.12347960472106934
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  2.2468361854553223
self.buckets_partition() spend  sec:  0.13570785522460938
input layer
dataloader gen time  12.45016860961914
weights_list [0.08493114447197196, 0.08506849942260049, 0.08395439815639133, 0.0870270792741554, 0.08625382177432073, 0.0849972783370894, 0.08370003713670887, 0.08103942087083038, 0.08075453652878603, 0.08072401320642414, 0.08071383876563684, 0.08083593205508442]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
pure train time  2.176619291305542
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 22.98779296875 GB
    Memory Allocated: 0.5944766998291016  GigaBytes
Max Memory Allocated: 21.26040267944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.394843339920044
epoch  7
load pickle file time  0.44176197052001953
the output layer 
self.num_batch (get_in_degree_bucketing) 12
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  12
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1464
G_BUCKET_ID_list [[11, 3, 2], [19, 13, 0], [23, 8], [14, 9, 7], [17, 10, 4], [6, 5, 1], [16, 12], [22], [21], [20], [18], [15]]
G_BUCKET_ID_list length 12
backpack scheduling spend  0.2984766960144043
len(g_bucket_nids_list)  12
len(local_split_batches_nid_list)  12
current group_mem  1.403347350375729
current group_mem  3.001967961271789
current group_mem  3.0000069573568977
current group_mem  3.0006915325832333
current group_mem  2.9975327611562568
current group_mem  1.0749997896799088
current group_mem  2.939728300182595
current group_mem  2.232189497508864
current group_mem  2.1946883605045633
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.4645761989081698
batches output list generation spend  0.0005970001220703125
self.weights_list  [0.08493114447197196, 0.08506849942260049, 0.08395439815639133, 0.0870270792741554, 0.08625382177432073, 0.0849972783370894, 0.08370003713670887, 0.08103942087083038, 0.08075453652878603, 0.08072401320642414, 0.08071383876563684, 0.08083593205508442]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.009736061096191406
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.299222469329834
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  2.434150218963623
self.buckets_partition() spend  sec:  0.30898380279541016
input layer
dataloader gen time  12.879039764404297
weights_list [0.08493114447197196, 0.08506849942260049, 0.08395439815639133, 0.0870270792741554, 0.08625382177432073, 0.0849972783370894, 0.08370003713670887, 0.08103942087083038, 0.08075453652878603, 0.08072401320642414, 0.08071383876563684, 0.08083593205508442]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
pure train time  2.173227071762085
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 22.98779296875 GB
    Memory Allocated: 0.594642162322998  GigaBytes
Max Memory Allocated: 21.26040267944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.2714028358459473
epoch  8
load pickle file time  0.43045806884765625
the output layer 
self.num_batch (get_in_degree_bucketing) 12
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  12
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1464
G_BUCKET_ID_list [[11, 3, 2], [19, 13, 0], [23, 8], [14, 9, 7], [17, 10, 4], [6, 5, 1], [16, 12], [22], [21], [20], [18], [15]]
G_BUCKET_ID_list length 12
backpack scheduling spend  0.1226658821105957
len(g_bucket_nids_list)  12
len(local_split_batches_nid_list)  12
current group_mem  1.403347350375729
current group_mem  3.001967961271789
current group_mem  3.0000069573568977
current group_mem  3.0006915325832333
current group_mem  2.9975327611562568
current group_mem  1.0749997896799088
current group_mem  2.939728300182595
current group_mem  2.232189497508864
current group_mem  2.1946883605045633
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.4645761989081698
batches output list generation spend  0.00043964385986328125
self.weights_list  [0.08493114447197196, 0.08506849942260049, 0.08395439815639133, 0.0870270792741554, 0.08625382177432073, 0.0849972783370894, 0.08370003713670887, 0.08103942087083038, 0.08075453652878603, 0.08072401320642414, 0.08071383876563684, 0.08083593205508442]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.009285926818847656
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.1232304573059082
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  2.2498137950897217
self.buckets_partition() spend  sec:  0.13253474235534668
input layer
dataloader gen time  13.264422416687012
weights_list [0.08493114447197196, 0.08506849942260049, 0.08395439815639133, 0.0870270792741554, 0.08625382177432073, 0.0849972783370894, 0.08370003713670887, 0.08103942087083038, 0.08075453652878603, 0.08072401320642414, 0.08071383876563684, 0.08083593205508442]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
pure train time  2.167292594909668
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 22.98779296875 GB
    Memory Allocated: 0.5951533317565918  GigaBytes
Max Memory Allocated: 21.26040267944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.1767510175704956
epoch  9
load pickle file time  0.4800856113433838
the output layer 
self.num_batch (get_in_degree_bucketing) 12
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  12
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1464
G_BUCKET_ID_list [[11, 3, 2], [19, 13, 0], [23, 8], [14, 9, 7], [17, 10, 4], [6, 5, 1], [16, 12], [22], [21], [20], [18], [15]]
G_BUCKET_ID_list length 12
backpack scheduling spend  0.12419700622558594
len(g_bucket_nids_list)  12
len(local_split_batches_nid_list)  12
current group_mem  1.403347350375729
current group_mem  3.001967961271789
current group_mem  3.0000069573568977
current group_mem  3.0006915325832333
current group_mem  2.9975327611562568
current group_mem  1.0749997896799088
current group_mem  2.939728300182595
current group_mem  2.232189497508864
current group_mem  2.1946883605045633
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.4645761989081698
batches output list generation spend  0.0008289813995361328
self.weights_list  [0.08493114447197196, 0.08506849942260049, 0.08395439815639133, 0.0870270792741554, 0.08625382177432073, 0.0849972783370894, 0.08370003713670887, 0.08103942087083038, 0.08075453652878603, 0.08072401320642414, 0.08071383876563684, 0.08083593205508442]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.010158300399780273
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.1251974105834961
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  2.2450320720672607
self.buckets_partition() spend  sec:  0.1353778839111328
input layer
dataloader gen time  12.346731185913086
weights_list [0.08493114447197196, 0.08506849942260049, 0.08395439815639133, 0.0870270792741554, 0.08625382177432073, 0.0849972783370894, 0.08370003713670887, 0.08103942087083038, 0.08075453652878603, 0.08072401320642414, 0.08071383876563684, 0.08083593205508442]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
pure train time  2.1663706302642822
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 22.98779296875 GB
    Memory Allocated: 0.5960545539855957  GigaBytes
Max Memory Allocated: 21.26040267944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.0826442241668701
epoch  10
load pickle file time  0.31094813346862793
the output layer 
self.num_batch (get_in_degree_bucketing) 12
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  12
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1464
G_BUCKET_ID_list [[11, 3, 2], [19, 13, 0], [23, 8], [14, 9, 7], [17, 10, 4], [6, 5, 1], [16, 12], [22], [21], [20], [18], [15]]
G_BUCKET_ID_list length 12
backpack scheduling spend  0.12302851676940918
len(g_bucket_nids_list)  12
len(local_split_batches_nid_list)  12
current group_mem  1.403347350375729
current group_mem  3.001967961271789
current group_mem  3.0000069573568977
current group_mem  3.0006915325832333
current group_mem  2.9975327611562568
current group_mem  1.0749997896799088
current group_mem  2.939728300182595
current group_mem  2.232189497508864
current group_mem  2.1946883605045633
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.4645761989081698
batches output list generation spend  0.0009276866912841797
self.weights_list  [0.08493114447197196, 0.08506849942260049, 0.08395439815639133, 0.0870270792741554, 0.08625382177432073, 0.0849972783370894, 0.08370003713670887, 0.08103942087083038, 0.08075453652878603, 0.08072401320642414, 0.08071383876563684, 0.08083593205508442]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.011254549026489258
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.12408304214477539
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  2.2603330612182617
self.buckets_partition() spend  sec:  0.13535690307617188
input layer
dataloader gen time  12.522219181060791
weights_list [0.08493114447197196, 0.08506849942260049, 0.08395439815639133, 0.0870270792741554, 0.08625382177432073, 0.0849972783370894, 0.08370003713670887, 0.08103942087083038, 0.08075453652878603, 0.08072401320642414, 0.08071383876563684, 0.08083593205508442]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
pure train time  2.179593801498413
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 22.98779296875 GB
    Memory Allocated: 0.5957961082458496  GigaBytes
Max Memory Allocated: 21.26040267944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.005535364151001
epoch  11
load pickle file time  0.3586616516113281
the output layer 
self.num_batch (get_in_degree_bucketing) 12
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  12
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1464
G_BUCKET_ID_list [[11, 3, 2], [19, 13, 0], [23, 8], [14, 9, 7], [17, 10, 4], [6, 5, 1], [16, 12], [22], [21], [20], [18], [15]]
G_BUCKET_ID_list length 12
backpack scheduling spend  0.1237947940826416
len(g_bucket_nids_list)  12
len(local_split_batches_nid_list)  12
current group_mem  1.403347350375729
current group_mem  3.001967961271789
current group_mem  3.0000069573568977
current group_mem  3.0006915325832333
current group_mem  2.9975327611562568
current group_mem  1.0749997896799088
current group_mem  2.939728300182595
current group_mem  2.232189497508864
current group_mem  2.1946883605045633
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.4645761989081698
batches output list generation spend  0.00048041343688964844
self.weights_list  [0.08493114447197196, 0.08506849942260049, 0.08395439815639133, 0.0870270792741554, 0.08625382177432073, 0.0849972783370894, 0.08370003713670887, 0.08103942087083038, 0.08075453652878603, 0.08072401320642414, 0.08071383876563684, 0.08083593205508442]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.010809659957885742
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.12442636489868164
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  2.262458324432373
self.buckets_partition() spend  sec:  0.1352541446685791
input layer
dataloader gen time  12.566958904266357
weights_list [0.08493114447197196, 0.08506849942260049, 0.08395439815639133, 0.0870270792741554, 0.08625382177432073, 0.0849972783370894, 0.08370003713670887, 0.08103942087083038, 0.08075453652878603, 0.08072401320642414, 0.08071383876563684, 0.08083593205508442]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
pure train time  2.171572685241699
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 22.98779296875 GB
    Memory Allocated: 0.5952644348144531  GigaBytes
Max Memory Allocated: 21.26040267944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.9573013782501221
epoch  12
load pickle file time  0.4914572238922119
the output layer 
self.num_batch (get_in_degree_bucketing) 12
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  12
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1464
G_BUCKET_ID_list [[11, 3, 2], [19, 13, 0], [23, 8], [14, 9, 7], [17, 10, 4], [6, 5, 1], [16, 12], [22], [21], [20], [18], [15]]
G_BUCKET_ID_list length 12
backpack scheduling spend  0.12402844429016113
len(g_bucket_nids_list)  12
len(local_split_batches_nid_list)  12
current group_mem  1.403347350375729
current group_mem  3.001967961271789
current group_mem  3.0000069573568977
current group_mem  3.0006915325832333
current group_mem  2.9975327611562568
current group_mem  1.0749997896799088
current group_mem  2.939728300182595
current group_mem  2.232189497508864
current group_mem  2.1946883605045633
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.4645761989081698
batches output list generation spend  0.0006060600280761719
self.weights_list  [0.08493114447197196, 0.08506849942260049, 0.08395439815639133, 0.0870270792741554, 0.08625382177432073, 0.0849972783370894, 0.08370003713670887, 0.08103942087083038, 0.08075453652878603, 0.08072401320642414, 0.08071383876563684, 0.08083593205508442]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.009905815124511719
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.12482762336730957
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  2.2590420246124268
self.buckets_partition() spend  sec:  0.13475251197814941
input layer
dataloader gen time  12.568210363388062
weights_list [0.08493114447197196, 0.08506849942260049, 0.08395439815639133, 0.0870270792741554, 0.08625382177432073, 0.0849972783370894, 0.08370003713670887, 0.08103942087083038, 0.08075453652878603, 0.08072401320642414, 0.08071383876563684, 0.08083593205508442]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
pure train time  2.167738914489746
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 22.98779296875 GB
    Memory Allocated: 0.5937972068786621  GigaBytes
Max Memory Allocated: 21.26040267944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.9118772745132446
epoch  13
load pickle file time  0.39077258110046387
the output layer 
self.num_batch (get_in_degree_bucketing) 12
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  12
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1464
G_BUCKET_ID_list [[11, 3, 2], [19, 13, 0], [23, 8], [14, 9, 7], [17, 10, 4], [6, 5, 1], [16, 12], [22], [21], [20], [18], [15]]
G_BUCKET_ID_list length 12
backpack scheduling spend  0.12285995483398438
len(g_bucket_nids_list)  12
len(local_split_batches_nid_list)  12
current group_mem  1.403347350375729
current group_mem  3.001967961271789
current group_mem  3.0000069573568977
current group_mem  3.0006915325832333
current group_mem  2.9975327611562568
current group_mem  1.0749997896799088
current group_mem  2.939728300182595
current group_mem  2.232189497508864
current group_mem  2.1946883605045633
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.4645761989081698
batches output list generation spend  0.00044345855712890625
self.weights_list  [0.08493114447197196, 0.08506849942260049, 0.08395439815639133, 0.0870270792741554, 0.08625382177432073, 0.0849972783370894, 0.08370003713670887, 0.08103942087083038, 0.08075453652878603, 0.08072401320642414, 0.08071383876563684, 0.08083593205508442]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.008914470672607422
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.12344169616699219
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  2.2479827404022217
self.buckets_partition() spend  sec:  0.13237571716308594
input layer
dataloader gen time  12.614108324050903
weights_list [0.08493114447197196, 0.08506849942260049, 0.08395439815639133, 0.0870270792741554, 0.08625382177432073, 0.0849972783370894, 0.08370003713670887, 0.08103942087083038, 0.08075453652878603, 0.08072401320642414, 0.08071383876563684, 0.08083593205508442]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
pure train time  2.1671090126037598
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 22.98779296875 GB
    Memory Allocated: 0.5923290252685547  GigaBytes
Max Memory Allocated: 21.26040267944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.8715550303459167
epoch  14
load pickle file time  0.33347058296203613
the output layer 
self.num_batch (get_in_degree_bucketing) 12
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  12
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1464
G_BUCKET_ID_list [[11, 3, 2], [19, 13, 0], [23, 8], [14, 9, 7], [17, 10, 4], [6, 5, 1], [16, 12], [22], [21], [20], [18], [15]]
G_BUCKET_ID_list length 12
backpack scheduling spend  0.1239311695098877
len(g_bucket_nids_list)  12
len(local_split_batches_nid_list)  12
current group_mem  1.403347350375729
current group_mem  3.001967961271789
current group_mem  3.0000069573568977
current group_mem  3.0006915325832333
current group_mem  2.9975327611562568
current group_mem  1.0749997896799088
current group_mem  2.939728300182595
current group_mem  2.232189497508864
current group_mem  2.1946883605045633
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.4645761989081698
batches output list generation spend  0.00043320655822753906
self.weights_list  [0.08493114447197196, 0.08506849942260049, 0.08395439815639133, 0.0870270792741554, 0.08625382177432073, 0.0849972783370894, 0.08370003713670887, 0.08103942087083038, 0.08075453652878603, 0.08072401320642414, 0.08071383876563684, 0.08083593205508442]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.00964665412902832
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.12449073791503906
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  2.245570659637451
self.buckets_partition() spend  sec:  0.1341555118560791
input layer
dataloader gen time  12.789256811141968
weights_list [0.08493114447197196, 0.08506849942260049, 0.08395439815639133, 0.0870270792741554, 0.08625382177432073, 0.0849972783370894, 0.08370003713670887, 0.08103942087083038, 0.08075453652878603, 0.08072401320642414, 0.08071383876563684, 0.08083593205508442]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
pure train time  2.1607666015625
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 22.98779296875 GB
    Memory Allocated: 0.5930399894714355  GigaBytes
Max Memory Allocated: 21.26040267944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.8364739418029785
epoch  15
load pickle file time  0.3021821975708008
the output layer 
self.num_batch (get_in_degree_bucketing) 12
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  12
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1464
G_BUCKET_ID_list [[11, 3, 2], [19, 13, 0], [23, 8], [14, 9, 7], [17, 10, 4], [6, 5, 1], [16, 12], [22], [21], [20], [18], [15]]
G_BUCKET_ID_list length 12
backpack scheduling spend  0.12392330169677734
len(g_bucket_nids_list)  12
len(local_split_batches_nid_list)  12
current group_mem  1.403347350375729
current group_mem  3.001967961271789
current group_mem  3.0000069573568977
current group_mem  3.0006915325832333
current group_mem  2.9975327611562568
current group_mem  1.0749997896799088
current group_mem  2.939728300182595
current group_mem  2.232189497508864
current group_mem  2.1946883605045633
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.4645761989081698
batches output list generation spend  0.0003972053527832031
self.weights_list  [0.08493114447197196, 0.08506849942260049, 0.08395439815639133, 0.0870270792741554, 0.08625382177432073, 0.0849972783370894, 0.08370003713670887, 0.08103942087083038, 0.08075453652878603, 0.08072401320642414, 0.08071383876563684, 0.08083593205508442]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.010076284408569336
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.12444663047790527
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  2.2496464252471924
self.buckets_partition() spend  sec:  0.13454103469848633
input layer
dataloader gen time  12.383601188659668
weights_list [0.08493114447197196, 0.08506849942260049, 0.08395439815639133, 0.0870270792741554, 0.08625382177432073, 0.0849972783370894, 0.08370003713670887, 0.08103942087083038, 0.08075453652878603, 0.08072401320642414, 0.08071383876563684, 0.08083593205508442]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
pure train time  2.167865514755249
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 22.98779296875 GB
    Memory Allocated: 0.5922122001647949  GigaBytes
Max Memory Allocated: 21.26040267944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.7970275282859802
epoch  16
load pickle file time  0.2924935817718506
the output layer 
self.num_batch (get_in_degree_bucketing) 12
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  12
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1464
G_BUCKET_ID_list [[11, 3, 2], [19, 13, 0], [23, 8], [14, 9, 7], [17, 10, 4], [6, 5, 1], [16, 12], [22], [21], [20], [18], [15]]
G_BUCKET_ID_list length 12
backpack scheduling spend  0.12395381927490234
len(g_bucket_nids_list)  12
len(local_split_batches_nid_list)  12
current group_mem  1.403347350375729
current group_mem  3.001967961271789
current group_mem  3.0000069573568977
current group_mem  3.0006915325832333
current group_mem  2.9975327611562568
current group_mem  1.0749997896799088
current group_mem  2.939728300182595
current group_mem  2.232189497508864
current group_mem  2.1946883605045633
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.4645761989081698
batches output list generation spend  0.00044035911560058594
self.weights_list  [0.08493114447197196, 0.08506849942260049, 0.08395439815639133, 0.0870270792741554, 0.08625382177432073, 0.0849972783370894, 0.08370003713670887, 0.08103942087083038, 0.08075453652878603, 0.08072401320642414, 0.08071383876563684, 0.08083593205508442]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.010086774826049805
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.12454342842102051
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  2.248692274093628
self.buckets_partition() spend  sec:  0.13465023040771484
input layer
dataloader gen time  12.369059085845947
weights_list [0.08493114447197196, 0.08506849942260049, 0.08395439815639133, 0.0870270792741554, 0.08625382177432073, 0.0849972783370894, 0.08370003713670887, 0.08103942087083038, 0.08075453652878603, 0.08072401320642414, 0.08071383876563684, 0.08083593205508442]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
pure train time  2.1718533039093018
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 22.98779296875 GB
    Memory Allocated: 0.5935463905334473  GigaBytes
Max Memory Allocated: 21.26040267944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.7631681561470032
epoch  17
load pickle file time  0.3040027618408203
the output layer 
self.num_batch (get_in_degree_bucketing) 12
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  12
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1464
G_BUCKET_ID_list [[11, 3, 2], [19, 13, 0], [23, 8], [14, 9, 7], [17, 10, 4], [6, 5, 1], [16, 12], [22], [21], [20], [18], [15]]
G_BUCKET_ID_list length 12
backpack scheduling spend  0.12334632873535156
len(g_bucket_nids_list)  12
len(local_split_batches_nid_list)  12
current group_mem  1.403347350375729
current group_mem  3.001967961271789
current group_mem  3.0000069573568977
current group_mem  3.0006915325832333
current group_mem  2.9975327611562568
current group_mem  1.0749997896799088
current group_mem  2.939728300182595
current group_mem  2.232189497508864
current group_mem  2.1946883605045633
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.4645761989081698
batches output list generation spend  0.0005578994750976562
self.weights_list  [0.08493114447197196, 0.08506849942260049, 0.08395439815639133, 0.0870270792741554, 0.08625382177432073, 0.0849972783370894, 0.08370003713670887, 0.08103942087083038, 0.08075453652878603, 0.08072401320642414, 0.08071383876563684, 0.08083593205508442]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.009115219116210938
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.12405061721801758
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  2.2550904750823975
self.buckets_partition() spend  sec:  0.13318562507629395
input layer
dataloader gen time  12.485406875610352
weights_list [0.08493114447197196, 0.08506849942260049, 0.08395439815639133, 0.0870270792741554, 0.08625382177432073, 0.0849972783370894, 0.08370003713670887, 0.08103942087083038, 0.08075453652878603, 0.08072401320642414, 0.08071383876563684, 0.08083593205508442]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
pure train time  2.1723451614379883
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 22.98779296875 GB
    Memory Allocated: 0.5931730270385742  GigaBytes
Max Memory Allocated: 21.26040267944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.7269291281700134
epoch  18
load pickle file time  0.29961204528808594
the output layer 
self.num_batch (get_in_degree_bucketing) 12
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  12
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1464
G_BUCKET_ID_list [[11, 3, 2], [19, 13, 0], [23, 8], [14, 9, 7], [17, 10, 4], [6, 5, 1], [16, 12], [22], [21], [20], [18], [15]]
G_BUCKET_ID_list length 12
backpack scheduling spend  0.12429404258728027
len(g_bucket_nids_list)  12
len(local_split_batches_nid_list)  12
current group_mem  1.403347350375729
current group_mem  3.001967961271789
current group_mem  3.0000069573568977
current group_mem  3.0006915325832333
current group_mem  2.9975327611562568
current group_mem  1.0749997896799088
current group_mem  2.939728300182595
current group_mem  2.232189497508864
current group_mem  2.1946883605045633
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.4645761989081698
batches output list generation spend  0.00045752525329589844
self.weights_list  [0.08493114447197196, 0.08506849942260049, 0.08395439815639133, 0.0870270792741554, 0.08625382177432073, 0.0849972783370894, 0.08370003713670887, 0.08103942087083038, 0.08075453652878603, 0.08072401320642414, 0.08071383876563684, 0.08083593205508442]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.012428522109985352
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.12487959861755371
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  2.257504940032959
self.buckets_partition() spend  sec:  0.13732695579528809
input layer
dataloader gen time  12.565364837646484
weights_list [0.08493114447197196, 0.08506849942260049, 0.08395439815639133, 0.0870270792741554, 0.08625382177432073, 0.0849972783370894, 0.08370003713670887, 0.08103942087083038, 0.08075453652878603, 0.08072401320642414, 0.08071383876563684, 0.08083593205508442]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
pure train time  2.171419620513916
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 22.98779296875 GB
    Memory Allocated: 0.593268871307373  GigaBytes
Max Memory Allocated: 21.26040267944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.7040643095970154
epoch  19
load pickle file time  0.3190310001373291
the output layer 
self.num_batch (get_in_degree_bucketing) 12
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  12
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1464
G_BUCKET_ID_list [[11, 3, 2], [19, 13, 0], [23, 8], [14, 9, 7], [17, 10, 4], [6, 5, 1], [16, 12], [22], [21], [20], [18], [15]]
G_BUCKET_ID_list length 12
backpack scheduling spend  0.12304449081420898
len(g_bucket_nids_list)  12
len(local_split_batches_nid_list)  12
current group_mem  1.403347350375729
current group_mem  3.001967961271789
current group_mem  3.0000069573568977
current group_mem  3.0006915325832333
current group_mem  2.9975327611562568
current group_mem  1.0749997896799088
current group_mem  2.939728300182595
current group_mem  2.232189497508864
current group_mem  2.1946883605045633
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.4645761989081698
batches output list generation spend  0.000431060791015625
self.weights_list  [0.08493114447197196, 0.08506849942260049, 0.08395439815639133, 0.0870270792741554, 0.08625382177432073, 0.0849972783370894, 0.08370003713670887, 0.08103942087083038, 0.08075453652878603, 0.08072401320642414, 0.08071383876563684, 0.08083593205508442]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.009530067443847656
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.12360215187072754
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  2.25205135345459
self.buckets_partition() spend  sec:  0.13315129280090332
input layer
dataloader gen time  12.635006189346313
weights_list [0.08493114447197196, 0.08506849942260049, 0.08395439815639133, 0.0870270792741554, 0.08625382177432073, 0.0849972783370894, 0.08370003713670887, 0.08103942087083038, 0.08075453652878603, 0.08072401320642414, 0.08071383876563684, 0.08083593205508442]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
pure train time  2.164534091949463
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 22.98779296875 GB
    Memory Allocated: 0.5925078392028809  GigaBytes
Max Memory Allocated: 21.26040267944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.6720635890960693
Total (block generation + training)time/epoch 18.493613663841696
pure train time/epoch 2.1700076907873154
dataloader time  [13.357699871063232, 12.74220585823059, 13.20257043838501, 12.45016860961914, 12.879039764404297, 13.264422416687012, 12.346731185913086, 12.522219181060791, 12.566958904266357, 12.568210363388062, 12.614108324050903, 12.789256811141968, 12.383601188659668, 12.369059085845947, 12.485406875610352, 12.565364837646484, 12.635006189346313]
dataloader time avg per epoch 12.614568086770864

num_input_list  [12586103, 12583576, 12580756, 12586729, 12584981, 12579751, 12588539, 12583005, 12582038, 12580723, 12582939, 12587416, 12581294, 12581591, 12579357, 12578742, 12581715, 12579822, 12581170, 12572220]
