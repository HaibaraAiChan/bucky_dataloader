main start at this time 1712916264.8910615
-----------------------------------------before load data 
 Nvidia-smi: 0.3560791015625 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

#nodes: 2449029
#edges: 123718024
#classes: 47
success----------------------------------------
196571
39255
2164782
# Nodes: 2400608
# Edges: 123718024
# Train: 196571
# Val: 39255
# Test: 2164782
# Classes: 47

----------------------------------------start of run function 
 Nvidia-smi: 0.3560791015625 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

labels  tensor([0, 1, 2,  ..., 8, 2, 4])
epoch  0
the output layer 
self.num_batch (get_in_degree_bucketing) 24
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  24
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.10846519470214844
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  24
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.00022792816162109375
self.weights_list  [0.04216796984295751, 0.04844559980872051, 0.04276317462901445, 0.042152708181776556, 0.04566289025339445, 0.04680242762157185, 0.04723992857542567, 0.04185256217855126, 0.04184238773776396, 0.04178134109304017, 0.04169994556674179, 0.04194413214563694, 0.041995004349573435, 0.042000091569967084, 0.041837300517370314, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03884092770551099]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.010126352310180664
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.10898923873901367
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  4.3753557205200195
self.buckets_partition() spend  sec:  0.11913633346557617
input layer
dataloader gen time  16.720014095306396
weights_list [0.04216796984295751, 0.04844559980872051, 0.04276317462901445, 0.042152708181776556, 0.04566289025339445, 0.04680242762157185, 0.04723992857542567, 0.04185256217855126, 0.04184238773776396, 0.04178134109304017, 0.04169994556674179, 0.04194413214563694, 0.041995004349573435, 0.042000091569967084, 0.041837300517370314, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03884092770551099]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
step  20
step  21
step  22
step  23
pure train time  5.442885875701904
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 14.13037109375 GB
    Memory Allocated: 0.4515872001647949  GigaBytes
Max Memory Allocated: 12.111384868621826  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 5.206521987915039
epoch  1
the output layer 
self.num_batch (get_in_degree_bucketing) 24
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  24
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.10137271881103516
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  24
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0003457069396972656
self.weights_list  [0.04216796984295751, 0.04844559980872051, 0.04276317462901445, 0.042152708181776556, 0.04566289025339445, 0.04680242762157185, 0.04723992857542567, 0.04185256217855126, 0.04184238773776396, 0.04178134109304017, 0.04169994556674179, 0.04194413214563694, 0.041995004349573435, 0.042000091569967084, 0.041837300517370314, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03884092770551099]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.010918855667114258
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.10191607475280762
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  4.332893133163452
self.buckets_partition() spend  sec:  0.11285710334777832
input layer
dataloader gen time  15.479533433914185
weights_list [0.04216796984295751, 0.04844559980872051, 0.04276317462901445, 0.042152708181776556, 0.04566289025339445, 0.04680242762157185, 0.04723992857542567, 0.04185256217855126, 0.04184238773776396, 0.04178134109304017, 0.04169994556674179, 0.04194413214563694, 0.041995004349573435, 0.042000091569967084, 0.041837300517370314, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03884092770551099]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
step  20
step  21
step  22
step  23
pure train time  3.0558860301971436
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 14.13818359375 GB
    Memory Allocated: 0.45072221755981445  GigaBytes
Max Memory Allocated: 12.157648086547852  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 3.4768271446228027
epoch  2
the output layer 
self.num_batch (get_in_degree_bucketing) 24
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  24
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.11753487586975098
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  24
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0009918212890625
self.weights_list  [0.04216796984295751, 0.04844559980872051, 0.04276317462901445, 0.042152708181776556, 0.04566289025339445, 0.04680242762157185, 0.04723992857542567, 0.04185256217855126, 0.04184238773776396, 0.04178134109304017, 0.04169994556674179, 0.04194413214563694, 0.041995004349573435, 0.042000091569967084, 0.041837300517370314, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03884092770551099]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.013907194137573242
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11880970001220703
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  4.50753116607666
self.buckets_partition() spend  sec:  0.13274240493774414
input layer
dataloader gen time  20.147146940231323
weights_list [0.04216796984295751, 0.04844559980872051, 0.04276317462901445, 0.042152708181776556, 0.04566289025339445, 0.04680242762157185, 0.04723992857542567, 0.04185256217855126, 0.04184238773776396, 0.04178134109304017, 0.04169994556674179, 0.04194413214563694, 0.041995004349573435, 0.042000091569967084, 0.041837300517370314, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03884092770551099]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
step  20
step  21
step  22
step  23
pure train time  2.990858316421509
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 14.13818359375 GB
    Memory Allocated: 0.45344018936157227  GigaBytes
Max Memory Allocated: 12.157648086547852  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 2.7361788749694824
epoch  3
load pickle file time  0.5690040588378906
the output layer 
self.num_batch (get_in_degree_bucketing) 24
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  24
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.10454940795898438
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  24
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0005650520324707031
self.weights_list  [0.04216796984295751, 0.04844559980872051, 0.04276317462901445, 0.042152708181776556, 0.04566289025339445, 0.04680242762157185, 0.04723992857542567, 0.04185256217855126, 0.04184238773776396, 0.04178134109304017, 0.04169994556674179, 0.04194413214563694, 0.041995004349573435, 0.042000091569967084, 0.041837300517370314, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03884092770551099]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.012611865997314453
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.10531949996948242
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  4.359121799468994
self.buckets_partition() spend  sec:  0.11795568466186523
input layer
dataloader gen time  17.072449684143066
weights_list [0.04216796984295751, 0.04844559980872051, 0.04276317462901445, 0.042152708181776556, 0.04566289025339445, 0.04680242762157185, 0.04723992857542567, 0.04185256217855126, 0.04184238773776396, 0.04178134109304017, 0.04169994556674179, 0.04194413214563694, 0.041995004349573435, 0.042000091569967084, 0.041837300517370314, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03884092770551099]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
step  20
step  21
step  22
step  23
pure train time  3.064412832260132
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 14.13818359375 GB
    Memory Allocated: 0.45180177688598633  GigaBytes
Max Memory Allocated: 12.157648086547852  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 2.2347893714904785
epoch  4
load pickle file time  0.6023063659667969
the output layer 
self.num_batch (get_in_degree_bucketing) 24
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  24
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.11282038688659668
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  24
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0006852149963378906
self.weights_list  [0.04216796984295751, 0.04844559980872051, 0.04276317462901445, 0.042152708181776556, 0.04566289025339445, 0.04680242762157185, 0.04723992857542567, 0.04185256217855126, 0.04184238773776396, 0.04178134109304017, 0.04169994556674179, 0.04194413214563694, 0.041995004349573435, 0.042000091569967084, 0.041837300517370314, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03884092770551099]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.01892828941345215
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11378192901611328
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  4.462666749954224
self.buckets_partition() spend  sec:  0.13274168968200684
input layer
dataloader gen time  16.73398184776306
weights_list [0.04216796984295751, 0.04844559980872051, 0.04276317462901445, 0.042152708181776556, 0.04566289025339445, 0.04680242762157185, 0.04723992857542567, 0.04185256217855126, 0.04184238773776396, 0.04178134109304017, 0.04169994556674179, 0.04194413214563694, 0.041995004349573435, 0.042000091569967084, 0.041837300517370314, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03884092770551099]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
step  20
step  21
step  22
step  23
pure train time  3.05263090133667
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 14.13818359375 GB
    Memory Allocated: 0.4520111083984375  GigaBytes
Max Memory Allocated: 12.157648086547852  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.776795506477356
epoch  5
load pickle file time  0.611919641494751
the output layer 
self.num_batch (get_in_degree_bucketing) 24
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  24
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.11150479316711426
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  24
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0005114078521728516
self.weights_list  [0.04216796984295751, 0.04844559980872051, 0.04276317462901445, 0.042152708181776556, 0.04566289025339445, 0.04680242762157185, 0.04723992857542567, 0.04185256217855126, 0.04184238773776396, 0.04178134109304017, 0.04169994556674179, 0.04194413214563694, 0.041995004349573435, 0.042000091569967084, 0.041837300517370314, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03884092770551099]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.02004241943359375
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.1121978759765625
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  4.393872499465942
self.buckets_partition() spend  sec:  0.13228440284729004
input layer
dataloader gen time  16.028269052505493
weights_list [0.04216796984295751, 0.04844559980872051, 0.04276317462901445, 0.042152708181776556, 0.04566289025339445, 0.04680242762157185, 0.04723992857542567, 0.04185256217855126, 0.04184238773776396, 0.04178134109304017, 0.04169994556674179, 0.04194413214563694, 0.041995004349573435, 0.042000091569967084, 0.041837300517370314, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03884092770551099]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
step  20
step  21
step  22
step  23
pure train time  3.297497034072876
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 14.13818359375 GB
    Memory Allocated: 0.4549827575683594  GigaBytes
Max Memory Allocated: 12.17130994796753  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5501264333724976
epoch  6
load pickle file time  0.6033625602722168
the output layer 
self.num_batch (get_in_degree_bucketing) 24
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  24
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.11107754707336426
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  24
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0005669593811035156
self.weights_list  [0.04216796984295751, 0.04844559980872051, 0.04276317462901445, 0.042152708181776556, 0.04566289025339445, 0.04680242762157185, 0.04723992857542567, 0.04185256217855126, 0.04184238773776396, 0.04178134109304017, 0.04169994556674179, 0.04194413214563694, 0.041995004349573435, 0.042000091569967084, 0.041837300517370314, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03884092770551099]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.014269113540649414
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11181306838989258
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  4.401520252227783
self.buckets_partition() spend  sec:  0.1261003017425537
input layer
dataloader gen time  17.767298698425293
weights_list [0.04216796984295751, 0.04844559980872051, 0.04276317462901445, 0.042152708181776556, 0.04566289025339445, 0.04680242762157185, 0.04723992857542567, 0.04185256217855126, 0.04184238773776396, 0.04178134109304017, 0.04169994556674179, 0.04194413214563694, 0.041995004349573435, 0.042000091569967084, 0.041837300517370314, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03884092770551099]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
step  20
step  21
step  22
step  23
pure train time  2.8768129348754883
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 14.13818359375 GB
    Memory Allocated: 0.45223426818847656  GigaBytes
Max Memory Allocated: 12.17130994796753  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.3904914855957031
epoch  7
load pickle file time  0.5477023124694824
the output layer 
self.num_batch (get_in_degree_bucketing) 24
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  24
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.10480952262878418
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  24
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0006127357482910156
self.weights_list  [0.04216796984295751, 0.04844559980872051, 0.04276317462901445, 0.042152708181776556, 0.04566289025339445, 0.04680242762157185, 0.04723992857542567, 0.04185256217855126, 0.04184238773776396, 0.04178134109304017, 0.04169994556674179, 0.04194413214563694, 0.041995004349573435, 0.042000091569967084, 0.041837300517370314, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03884092770551099]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.013124227523803711
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.10562539100646973
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  4.399807453155518
self.buckets_partition() spend  sec:  0.11877703666687012
input layer
dataloader gen time  19.066813230514526
weights_list [0.04216796984295751, 0.04844559980872051, 0.04276317462901445, 0.042152708181776556, 0.04566289025339445, 0.04680242762157185, 0.04723992857542567, 0.04185256217855126, 0.04184238773776396, 0.04178134109304017, 0.04169994556674179, 0.04194413214563694, 0.041995004349573435, 0.042000091569967084, 0.041837300517370314, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03884092770551099]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
step  20
step  21
step  22
step  23
pure train time  3.0495378971099854
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 14.14404296875 GB
    Memory Allocated: 0.45234155654907227  GigaBytes
Max Memory Allocated: 12.17130994796753  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.2748706340789795
epoch  8
load pickle file time  0.6199493408203125
the output layer 
self.num_batch (get_in_degree_bucketing) 24
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  24
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.11706876754760742
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  24
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.00041961669921875
self.weights_list  [0.04216796984295751, 0.04844559980872051, 0.04276317462901445, 0.042152708181776556, 0.04566289025339445, 0.04680242762157185, 0.04723992857542567, 0.04185256217855126, 0.04184238773776396, 0.04178134109304017, 0.04169994556674179, 0.04194413214563694, 0.041995004349573435, 0.042000091569967084, 0.041837300517370314, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03884092770551099]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.011899709701538086
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11770224571228027
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  4.451864242553711
self.buckets_partition() spend  sec:  0.12964200973510742
input layer
dataloader gen time  17.275913953781128
weights_list [0.04216796984295751, 0.04844559980872051, 0.04276317462901445, 0.042152708181776556, 0.04566289025339445, 0.04680242762157185, 0.04723992857542567, 0.04185256217855126, 0.04184238773776396, 0.04178134109304017, 0.04169994556674179, 0.04194413214563694, 0.041995004349573435, 0.042000091569967084, 0.041837300517370314, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03884092770551099]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
step  20
step  21
step  22
step  23
pure train time  3.0424718856811523
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 14.14404296875 GB
    Memory Allocated: 0.4515724182128906  GigaBytes
Max Memory Allocated: 12.17130994796753  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.1762686967849731
epoch  9
load pickle file time  0.5785064697265625
the output layer 
self.num_batch (get_in_degree_bucketing) 24
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  24
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.10587882995605469
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  24
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0005166530609130859
self.weights_list  [0.04216796984295751, 0.04844559980872051, 0.04276317462901445, 0.042152708181776556, 0.04566289025339445, 0.04680242762157185, 0.04723992857542567, 0.04185256217855126, 0.04184238773776396, 0.04178134109304017, 0.04169994556674179, 0.04194413214563694, 0.041995004349573435, 0.042000091569967084, 0.041837300517370314, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03884092770551099]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.012865304946899414
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.10661697387695312
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  4.457705020904541
self.buckets_partition() spend  sec:  0.1196296215057373
input layer
dataloader gen time  16.889429569244385
weights_list [0.04216796984295751, 0.04844559980872051, 0.04276317462901445, 0.042152708181776556, 0.04566289025339445, 0.04680242762157185, 0.04723992857542567, 0.04185256217855126, 0.04184238773776396, 0.04178134109304017, 0.04169994556674179, 0.04194413214563694, 0.041995004349573435, 0.042000091569967084, 0.041837300517370314, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03884092770551099]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
step  20
step  21
step  22
step  23
pure train time  3.0304665565490723
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 14.14404296875 GB
    Memory Allocated: 0.4520859718322754  GigaBytes
Max Memory Allocated: 12.17130994796753  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.08176589012146
epoch  10
load pickle file time  0.570946455001831
the output layer 
self.num_batch (get_in_degree_bucketing) 24
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  24
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.11555957794189453
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  24
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0005640983581542969
self.weights_list  [0.04216796984295751, 0.04844559980872051, 0.04276317462901445, 0.042152708181776556, 0.04566289025339445, 0.04680242762157185, 0.04723992857542567, 0.04185256217855126, 0.04184238773776396, 0.04178134109304017, 0.04169994556674179, 0.04194413214563694, 0.041995004349573435, 0.042000091569967084, 0.041837300517370314, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03884092770551099]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.012326955795288086
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11635947227478027
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  4.40602970123291
self.buckets_partition() spend  sec:  0.12871193885803223
input layer
dataloader gen time  16.807667016983032
weights_list [0.04216796984295751, 0.04844559980872051, 0.04276317462901445, 0.042152708181776556, 0.04566289025339445, 0.04680242762157185, 0.04723992857542567, 0.04185256217855126, 0.04184238773776396, 0.04178134109304017, 0.04169994556674179, 0.04194413214563694, 0.041995004349573435, 0.042000091569967084, 0.041837300517370314, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03884092770551099]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
step  20
step  21
step  22
step  23
pure train time  3.0220935344696045
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 14.14404296875 GB
    Memory Allocated: 0.45087718963623047  GigaBytes
Max Memory Allocated: 12.17130994796753  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.0030837059020996
epoch  11
load pickle file time  0.5351972579956055
the output layer 
self.num_batch (get_in_degree_bucketing) 24
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  24
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.10443830490112305
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  24
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0004405975341796875
self.weights_list  [0.04216796984295751, 0.04844559980872051, 0.04276317462901445, 0.042152708181776556, 0.04566289025339445, 0.04680242762157185, 0.04723992857542567, 0.04185256217855126, 0.04184238773776396, 0.04178134109304017, 0.04169994556674179, 0.04194413214563694, 0.041995004349573435, 0.042000091569967084, 0.041837300517370314, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03884092770551099]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.013993263244628906
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.1050405502319336
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  4.409190654754639
self.buckets_partition() spend  sec:  0.11905217170715332
input layer
dataloader gen time  16.794847011566162
weights_list [0.04216796984295751, 0.04844559980872051, 0.04276317462901445, 0.042152708181776556, 0.04566289025339445, 0.04680242762157185, 0.04723992857542567, 0.04185256217855126, 0.04184238773776396, 0.04178134109304017, 0.04169994556674179, 0.04194413214563694, 0.041995004349573435, 0.042000091569967084, 0.041837300517370314, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03884092770551099]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
step  20
step  21
step  22
step  23
pure train time  3.047325849533081
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 14.14404296875 GB
    Memory Allocated: 0.45229005813598633  GigaBytes
Max Memory Allocated: 12.17130994796753  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.9559146165847778
epoch  12
load pickle file time  0.6022520065307617
the output layer 
self.num_batch (get_in_degree_bucketing) 24
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  24
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.11359834671020508
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  24
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.00034332275390625
self.weights_list  [0.04216796984295751, 0.04844559980872051, 0.04276317462901445, 0.042152708181776556, 0.04566289025339445, 0.04680242762157185, 0.04723992857542567, 0.04185256217855126, 0.04184238773776396, 0.04178134109304017, 0.04169994556674179, 0.04194413214563694, 0.041995004349573435, 0.042000091569967084, 0.041837300517370314, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03884092770551099]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.011315584182739258
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11426687240600586
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  4.332139730453491
self.buckets_partition() spend  sec:  0.12562036514282227
input layer
dataloader gen time  16.05365824699402
weights_list [0.04216796984295751, 0.04844559980872051, 0.04276317462901445, 0.042152708181776556, 0.04566289025339445, 0.04680242762157185, 0.04723992857542567, 0.04185256217855126, 0.04184238773776396, 0.04178134109304017, 0.04169994556674179, 0.04194413214563694, 0.041995004349573435, 0.042000091569967084, 0.041837300517370314, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03884092770551099]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
step  20
step  21
step  22
step  23
pure train time  3.029461622238159
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 14.14404296875 GB
    Memory Allocated: 0.4520854949951172  GigaBytes
Max Memory Allocated: 12.17130994796753  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.9129414558410645
epoch  13
load pickle file time  0.5455276966094971
the output layer 
self.num_batch (get_in_degree_bucketing) 24
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  24
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.10389304161071777
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  24
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0005815029144287109
self.weights_list  [0.04216796984295751, 0.04844559980872051, 0.04276317462901445, 0.042152708181776556, 0.04566289025339445, 0.04680242762157185, 0.04723992857542567, 0.04185256217855126, 0.04184238773776396, 0.04178134109304017, 0.04169994556674179, 0.04194413214563694, 0.041995004349573435, 0.042000091569967084, 0.041837300517370314, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03884092770551099]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.01194620132446289
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.10465407371520996
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  4.451476573944092
self.buckets_partition() spend  sec:  0.1166226863861084
input layer
dataloader gen time  15.513185739517212
weights_list [0.04216796984295751, 0.04844559980872051, 0.04276317462901445, 0.042152708181776556, 0.04566289025339445, 0.04680242762157185, 0.04723992857542567, 0.04185256217855126, 0.04184238773776396, 0.04178134109304017, 0.04169994556674179, 0.04194413214563694, 0.041995004349573435, 0.042000091569967084, 0.041837300517370314, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03884092770551099]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
step  20
step  21
step  22
step  23
pure train time  3.0402891635894775
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 14.78076171875 GB
    Memory Allocated: 0.45155763626098633  GigaBytes
Max Memory Allocated: 12.17130994796753  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.8740794062614441
epoch  14
load pickle file time  0.5940778255462646
the output layer 
self.num_batch (get_in_degree_bucketing) 24
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  24
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.11386609077453613
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  24
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.00041365623474121094
self.weights_list  [0.04216796984295751, 0.04844559980872051, 0.04276317462901445, 0.042152708181776556, 0.04566289025339445, 0.04680242762157185, 0.04723992857542567, 0.04185256217855126, 0.04184238773776396, 0.04178134109304017, 0.04169994556674179, 0.04194413214563694, 0.041995004349573435, 0.042000091569967084, 0.041837300517370314, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03884092770551099]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.011911869049072266
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.1145331859588623
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  4.446734666824341
self.buckets_partition() spend  sec:  0.1264641284942627
input layer
dataloader gen time  16.008902072906494
weights_list [0.04216796984295751, 0.04844559980872051, 0.04276317462901445, 0.042152708181776556, 0.04566289025339445, 0.04680242762157185, 0.04723992857542567, 0.04185256217855126, 0.04184238773776396, 0.04178134109304017, 0.04169994556674179, 0.04194413214563694, 0.041995004349573435, 0.042000091569967084, 0.041837300517370314, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03884092770551099]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
step  20
step  21
step  22
step  23
pure train time  3.042823553085327
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 14.78076171875 GB
    Memory Allocated: 0.4516105651855469  GigaBytes
Max Memory Allocated: 12.17130994796753  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.8349741101264954
epoch  15
load pickle file time  0.5359675884246826
the output layer 
self.num_batch (get_in_degree_bucketing) 24
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  24
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.10414719581604004
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  24
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0004177093505859375
self.weights_list  [0.04216796984295751, 0.04844559980872051, 0.04276317462901445, 0.042152708181776556, 0.04566289025339445, 0.04680242762157185, 0.04723992857542567, 0.04185256217855126, 0.04184238773776396, 0.04178134109304017, 0.04169994556674179, 0.04194413214563694, 0.041995004349573435, 0.042000091569967084, 0.041837300517370314, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03884092770551099]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.01163339614868164
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.10474896430969238
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  4.462943077087402
self.buckets_partition() spend  sec:  0.11641263961791992
input layer
dataloader gen time  15.353055953979492
weights_list [0.04216796984295751, 0.04844559980872051, 0.04276317462901445, 0.042152708181776556, 0.04566289025339445, 0.04680242762157185, 0.04723992857542567, 0.04185256217855126, 0.04184238773776396, 0.04178134109304017, 0.04169994556674179, 0.04194413214563694, 0.041995004349573435, 0.042000091569967084, 0.041837300517370314, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03884092770551099]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
step  20
step  21
step  22
step  23
pure train time  3.021650791168213
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 14.78076171875 GB
    Memory Allocated: 0.45137500762939453  GigaBytes
Max Memory Allocated: 12.17130994796753  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.7989392876625061
epoch  16
load pickle file time  0.5732769966125488
the output layer 
self.num_batch (get_in_degree_bucketing) 24
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  24
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.11297941207885742
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  24
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.00043892860412597656
self.weights_list  [0.04216796984295751, 0.04844559980872051, 0.04276317462901445, 0.042152708181776556, 0.04566289025339445, 0.04680242762157185, 0.04723992857542567, 0.04185256217855126, 0.04184238773776396, 0.04178134109304017, 0.04169994556674179, 0.04194413214563694, 0.041995004349573435, 0.042000091569967084, 0.041837300517370314, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03884092770551099]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.012439966201782227
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11359024047851562
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  4.4615490436553955
self.buckets_partition() spend  sec:  0.12604999542236328
input layer
dataloader gen time  16.031989097595215
weights_list [0.04216796984295751, 0.04844559980872051, 0.04276317462901445, 0.042152708181776556, 0.04566289025339445, 0.04680242762157185, 0.04723992857542567, 0.04185256217855126, 0.04184238773776396, 0.04178134109304017, 0.04169994556674179, 0.04194413214563694, 0.041995004349573435, 0.042000091569967084, 0.041837300517370314, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03884092770551099]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
step  20
step  21
step  22
step  23
pure train time  3.0788984298706055
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 14.78076171875 GB
    Memory Allocated: 0.45185184478759766  GigaBytes
Max Memory Allocated: 12.17130994796753  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.7631076574325562
epoch  17
load pickle file time  0.5357632637023926
the output layer 
self.num_batch (get_in_degree_bucketing) 24
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  24
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.10557985305786133
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  24
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0003800392150878906
self.weights_list  [0.04216796984295751, 0.04844559980872051, 0.04276317462901445, 0.042152708181776556, 0.04566289025339445, 0.04680242762157185, 0.04723992857542567, 0.04185256217855126, 0.04184238773776396, 0.04178134109304017, 0.04169994556674179, 0.04194413214563694, 0.041995004349573435, 0.042000091569967084, 0.041837300517370314, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03884092770551099]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.010415077209472656
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.10613775253295898
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  4.426722526550293
self.buckets_partition() spend  sec:  0.1165764331817627
input layer
dataloader gen time  15.28722882270813
weights_list [0.04216796984295751, 0.04844559980872051, 0.04276317462901445, 0.042152708181776556, 0.04566289025339445, 0.04680242762157185, 0.04723992857542567, 0.04185256217855126, 0.04184238773776396, 0.04178134109304017, 0.04169994556674179, 0.04194413214563694, 0.041995004349573435, 0.042000091569967084, 0.041837300517370314, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03884092770551099]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
step  20
step  21
step  22
step  23
pure train time  3.1215460300445557
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 14.78076171875 GB
    Memory Allocated: 0.451901912689209  GigaBytes
Max Memory Allocated: 12.17130994796753  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.7303975820541382
epoch  18
load pickle file time  0.5784533023834229
the output layer 
self.num_batch (get_in_degree_bucketing) 24
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  24
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.11225414276123047
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  24
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0005385875701904297
self.weights_list  [0.04216796984295751, 0.04844559980872051, 0.04276317462901445, 0.042152708181776556, 0.04566289025339445, 0.04680242762157185, 0.04723992857542567, 0.04185256217855126, 0.04184238773776396, 0.04178134109304017, 0.04169994556674179, 0.04194413214563694, 0.041995004349573435, 0.042000091569967084, 0.041837300517370314, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03884092770551099]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.012359619140625
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11296534538269043
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  4.434843301773071
self.buckets_partition() spend  sec:  0.12534356117248535
input layer
dataloader gen time  15.608314037322998
weights_list [0.04216796984295751, 0.04844559980872051, 0.04276317462901445, 0.042152708181776556, 0.04566289025339445, 0.04680242762157185, 0.04723992857542567, 0.04185256217855126, 0.04184238773776396, 0.04178134109304017, 0.04169994556674179, 0.04194413214563694, 0.041995004349573435, 0.042000091569967084, 0.041837300517370314, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03884092770551099]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
step  20
step  21
step  22
step  23
pure train time  3.097144603729248
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 14.78076171875 GB
    Memory Allocated: 0.4517793655395508  GigaBytes
Max Memory Allocated: 12.17130994796753  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.7013893723487854
epoch  19
load pickle file time  0.5551154613494873
the output layer 
self.num_batch (get_in_degree_bucketing) 24
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  24
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.1044619083404541
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  24
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0004210472106933594
self.weights_list  [0.04216796984295751, 0.04844559980872051, 0.04276317462901445, 0.042152708181776556, 0.04566289025339445, 0.04680242762157185, 0.04723992857542567, 0.04185256217855126, 0.04184238773776396, 0.04178134109304017, 0.04169994556674179, 0.04194413214563694, 0.041995004349573435, 0.042000091569967084, 0.041837300517370314, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03884092770551099]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.01042628288269043
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.1050417423248291
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  4.476407289505005
self.buckets_partition() spend  sec:  0.11548662185668945
input layer
dataloader gen time  15.808280229568481
weights_list [0.04216796984295751, 0.04844559980872051, 0.04276317462901445, 0.042152708181776556, 0.04566289025339445, 0.04680242762157185, 0.04723992857542567, 0.04185256217855126, 0.04184238773776396, 0.04178134109304017, 0.04169994556674179, 0.04194413214563694, 0.041995004349573435, 0.042000091569967084, 0.041837300517370314, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03887145102787288, 0.03884092770551099]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
step  20
step  21
step  22
step  23
pure train time  3.117171287536621
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 14.78076171875 GB
    Memory Allocated: 0.45199012756347656  GigaBytes
Max Memory Allocated: 12.17130994796753  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.6729333996772766
Total (block generation + training)time/epoch 23.437063076916864
pure train time/epoch 3.0604888796806335
dataloader time  [17.072449684143066, 16.73398184776306, 16.028269052505493, 17.767298698425293, 19.066813230514526, 17.275913953781128, 16.889429569244385, 16.807667016983032, 16.794847011566162, 16.05365824699402, 15.513185739517212, 16.008902072906494, 15.353055953979492, 16.031989097595215, 15.28722882270813, 15.608314037322998, 15.808280229568481]
dataloader time avg per epoch 16.346098844821636

num_input_list  [18602526, 18606528, 18598542, 18606160, 18607677, 18600324, 18609910, 18596877, 18599015, 18596966, 18595131, 18604899, 18596761, 18587968, 18598404, 18588776, 18598279, 18594551, 18601093, 18587930]
