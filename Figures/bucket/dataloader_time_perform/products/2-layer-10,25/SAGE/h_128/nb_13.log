main start at this time 1712912275.6241355
-----------------------------------------before load data 
 Nvidia-smi: 0.3560791015625 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

#nodes: 2449029
#edges: 123718024
#classes: 47
success----------------------------------------
196571
39255
2164782
# Nodes: 2400608
# Edges: 123718024
# Train: 196571
# Val: 39255
# Test: 2164782
# Classes: 47

----------------------------------------start of run function 
 Nvidia-smi: 0.3560791015625 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

labels  tensor([0, 1, 2,  ..., 8, 2, 4])
epoch  0
the output layer 
self.num_batch (get_in_degree_bucketing) 13
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  13
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1599
G_BUCKET_ID_list [[18, 8, 3], [14, 13, 2], [9, 4, 1], [10, 11, 7], [23, 6, 0], [15, 12], [22, 5], [21], [20], [19], [16], [17]]
G_BUCKET_ID_list length 12
backpack scheduling spend  0.10872435569763184
len(g_bucket_nids_list)  12
len(local_split_batches_nid_list)  13
current group_mem  2.802429055094217
current group_mem  2.80263728300465
current group_mem  1.3379273555544262
current group_mem  2.8007725725464603
current group_mem  2.7972885987579055
current group_mem  2.7477890907641793
current group_mem  2.6876900115433844
current group_mem  2.1946883605045633
current group_mem  1.920127067184496
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
batches output list generation spend  0.00031256675720214844
self.weights_list  [0.07998127902895137, 0.08062226879855117, 0.07840424070692015, 0.08050526272949723, 0.07925380651265955, 0.07801252473660916, 0.07785482090440604, 0.07477196534585467, 0.07474144202349278, 0.0746702209379817, 0.07458882541168331, 0.07483301199057847, 0.0717603308728144]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.00939321517944336
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.10926675796508789
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  2.428440809249878
self.buckets_partition() spend  sec:  0.11868071556091309
input layer
dataloader gen time  14.749002456665039
weights_list [0.07998127902895137, 0.08062226879855117, 0.07840424070692015, 0.08050526272949723, 0.07925380651265955, 0.07801252473660916, 0.07785482090440604, 0.07477196534585467, 0.07474144202349278, 0.0746702209379817, 0.07458882541168331, 0.07483301199057847, 0.0717603308728144]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
pure train time  3.750786781311035
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 21.61669921875 GB
    Memory Allocated: 0.5749258995056152  GigaBytes
Max Memory Allocated: 19.924737453460693  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 5.212055206298828
epoch  1
the output layer 
self.num_batch (get_in_degree_bucketing) 13
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  13
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1599
G_BUCKET_ID_list [[18, 8, 3], [14, 13, 2], [9, 4, 1], [10, 11, 7], [23, 6, 0], [15, 12], [22, 5], [21], [20], [19], [16], [17]]
G_BUCKET_ID_list length 12
backpack scheduling spend  0.11225295066833496
len(g_bucket_nids_list)  12
len(local_split_batches_nid_list)  13
current group_mem  2.802429055094217
current group_mem  2.80263728300465
current group_mem  1.3379273555544262
current group_mem  2.8007725725464603
current group_mem  2.7972885987579055
current group_mem  2.7477890907641793
current group_mem  2.6876900115433844
current group_mem  2.1946883605045633
current group_mem  1.920127067184496
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
batches output list generation spend  0.00039649009704589844
self.weights_list  [0.07998127902895137, 0.08062226879855117, 0.07840424070692015, 0.08050526272949723, 0.07925380651265955, 0.07801252473660916, 0.07785482090440604, 0.07477196534585467, 0.07474144202349278, 0.0746702209379817, 0.07458882541168331, 0.07483301199057847, 0.0717603308728144]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.010560035705566406
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.1128079891204834
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  2.4071714878082275
self.buckets_partition() spend  sec:  0.12339067459106445
input layer
dataloader gen time  15.917172908782959
weights_list [0.07998127902895137, 0.08062226879855117, 0.07840424070692015, 0.08050526272949723, 0.07925380651265955, 0.07801252473660916, 0.07785482090440604, 0.07477196534585467, 0.07474144202349278, 0.0746702209379817, 0.07458882541168331, 0.07483301199057847, 0.0717603308728144]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
pure train time  2.2497799396514893
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 21.61865234375 GB
    Memory Allocated: 0.5742087364196777  GigaBytes
Max Memory Allocated: 19.924737453460693  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 3.4832682609558105
epoch  2
the output layer 
self.num_batch (get_in_degree_bucketing) 13
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  13
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1599
G_BUCKET_ID_list [[18, 8, 3], [14, 13, 2], [9, 4, 1], [10, 11, 7], [23, 6, 0], [15, 12], [22, 5], [21], [20], [19], [16], [17]]
G_BUCKET_ID_list length 12
backpack scheduling spend  0.11985945701599121
len(g_bucket_nids_list)  12
len(local_split_batches_nid_list)  13
current group_mem  2.802429055094217
current group_mem  2.80263728300465
current group_mem  1.3379273555544262
current group_mem  2.8007725725464603
current group_mem  2.7972885987579055
current group_mem  2.7477890907641793
current group_mem  2.6876900115433844
current group_mem  2.1946883605045633
current group_mem  1.920127067184496
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
batches output list generation spend  0.0016772747039794922
self.weights_list  [0.07998127902895137, 0.08062226879855117, 0.07840424070692015, 0.08050526272949723, 0.07925380651265955, 0.07801252473660916, 0.07785482090440604, 0.07477196534585467, 0.07474144202349278, 0.0746702209379817, 0.07458882541168331, 0.07483301199057847, 0.0717603308728144]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.02048468589782715
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.12193417549133301
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  2.5480241775512695
self.buckets_partition() spend  sec:  0.14245390892028809
input layer
dataloader gen time  17.697052478790283
weights_list [0.07998127902895137, 0.08062226879855117, 0.07840424070692015, 0.08050526272949723, 0.07925380651265955, 0.07801252473660916, 0.07785482090440604, 0.07477196534585467, 0.07474144202349278, 0.0746702209379817, 0.07458882541168331, 0.07483301199057847, 0.0717603308728144]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
pure train time  2.2967824935913086
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 22.64794921875 GB
    Memory Allocated: 0.5814070701599121  GigaBytes
Max Memory Allocated: 19.938227653503418  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 2.7350611686706543
epoch  3
load pickle file time  0.616269588470459
the output layer 
self.num_batch (get_in_degree_bucketing) 13
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  13
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1599
G_BUCKET_ID_list [[18, 8, 3], [14, 13, 2], [9, 4, 1], [10, 11, 7], [23, 6, 0], [15, 12], [22, 5], [21], [20], [19], [16], [17]]
G_BUCKET_ID_list length 12
backpack scheduling spend  0.12274694442749023
len(g_bucket_nids_list)  12
len(local_split_batches_nid_list)  13
current group_mem  2.802429055094217
current group_mem  2.80263728300465
current group_mem  1.3379273555544262
current group_mem  2.8007725725464603
current group_mem  2.7972885987579055
current group_mem  2.7477890907641793
current group_mem  2.6876900115433844
current group_mem  2.1946883605045633
current group_mem  1.920127067184496
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
batches output list generation spend  0.0018422603607177734
self.weights_list  [0.07998127902895137, 0.08062226879855117, 0.07840424070692015, 0.08050526272949723, 0.07925380651265955, 0.07801252473660916, 0.07785482090440604, 0.07477196534585467, 0.07474144202349278, 0.0746702209379817, 0.07458882541168331, 0.07483301199057847, 0.0717603308728144]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.016605377197265625
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.12491250038146973
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  2.4906418323516846
self.buckets_partition() spend  sec:  0.1415421962738037
input layer
dataloader gen time  16.473373413085938
weights_list [0.07998127902895137, 0.08062226879855117, 0.07840424070692015, 0.08050526272949723, 0.07925380651265955, 0.07801252473660916, 0.07785482090440604, 0.07477196534585467, 0.07474144202349278, 0.0746702209379817, 0.07458882541168331, 0.07483301199057847, 0.0717603308728144]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
pure train time  2.2540292739868164
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 23.70068359375 GB
    Memory Allocated: 0.5764307975769043  GigaBytes
Max Memory Allocated: 19.943279266357422  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 2.229579210281372
epoch  4
load pickle file time  0.5125782489776611
the output layer 
self.num_batch (get_in_degree_bucketing) 13
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  13
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1599
G_BUCKET_ID_list [[18, 8, 3], [14, 13, 2], [9, 4, 1], [10, 11, 7], [23, 6, 0], [15, 12], [22, 5], [21], [20], [19], [16], [17]]
G_BUCKET_ID_list length 12
backpack scheduling spend  0.12542009353637695
len(g_bucket_nids_list)  12
len(local_split_batches_nid_list)  13
current group_mem  2.802429055094217
current group_mem  2.80263728300465
current group_mem  1.3379273555544262
current group_mem  2.8007725725464603
current group_mem  2.7972885987579055
current group_mem  2.7477890907641793
current group_mem  2.6876900115433844
current group_mem  2.1946883605045633
current group_mem  1.920127067184496
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
batches output list generation spend  0.0005483627319335938
self.weights_list  [0.07998127902895137, 0.08062226879855117, 0.07840424070692015, 0.08050526272949723, 0.07925380651265955, 0.07801252473660916, 0.07785482090440604, 0.07477196534585467, 0.07474144202349278, 0.0746702209379817, 0.07458882541168331, 0.07483301199057847, 0.0717603308728144]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.01060938835144043
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.12612342834472656
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  2.506854295730591
self.buckets_partition() spend  sec:  0.13675999641418457
input layer
dataloader gen time  15.87657880783081
weights_list [0.07998127902895137, 0.08062226879855117, 0.07840424070692015, 0.08050526272949723, 0.07925380651265955, 0.07801252473660916, 0.07785482090440604, 0.07477196534585467, 0.07474144202349278, 0.0746702209379817, 0.07458882541168331, 0.07483301199057847, 0.0717603308728144]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
pure train time  2.2643744945526123
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 23.70263671875 GB
    Memory Allocated: 0.5749025344848633  GigaBytes
Max Memory Allocated: 19.943279266357422  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7767997980117798
epoch  5
load pickle file time  0.5927371978759766
the output layer 
self.num_batch (get_in_degree_bucketing) 13
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  13
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1599
G_BUCKET_ID_list [[18, 8, 3], [14, 13, 2], [9, 4, 1], [10, 11, 7], [23, 6, 0], [15, 12], [22, 5], [21], [20], [19], [16], [17]]
G_BUCKET_ID_list length 12
backpack scheduling spend  0.11636567115783691
len(g_bucket_nids_list)  12
len(local_split_batches_nid_list)  13
current group_mem  2.802429055094217
current group_mem  2.80263728300465
current group_mem  1.3379273555544262
current group_mem  2.8007725725464603
current group_mem  2.7972885987579055
current group_mem  2.7477890907641793
current group_mem  2.6876900115433844
current group_mem  2.1946883605045633
current group_mem  1.920127067184496
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
batches output list generation spend  0.0006499290466308594
self.weights_list  [0.07998127902895137, 0.08062226879855117, 0.07840424070692015, 0.08050526272949723, 0.07925380651265955, 0.07801252473660916, 0.07785482090440604, 0.07477196534585467, 0.07474144202349278, 0.0746702209379817, 0.07458882541168331, 0.07483301199057847, 0.0717603308728144]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.009394407272338867
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11717557907104492
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  2.475161552429199
self.buckets_partition() spend  sec:  0.12659049034118652
input layer
dataloader gen time  16.00684952735901
weights_list [0.07998127902895137, 0.08062226879855117, 0.07840424070692015, 0.08050526272949723, 0.07925380651265955, 0.07801252473660916, 0.07785482090440604, 0.07477196534585467, 0.07474144202349278, 0.0746702209379817, 0.07458882541168331, 0.07483301199057847, 0.0717603308728144]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
pure train time  2.2538907527923584
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 23.70263671875 GB
    Memory Allocated: 0.5782842636108398  GigaBytes
Max Memory Allocated: 19.943279266357422  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5464211702346802
epoch  6
load pickle file time  0.5479304790496826
the output layer 
self.num_batch (get_in_degree_bucketing) 13
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  13
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1599
G_BUCKET_ID_list [[18, 8, 3], [14, 13, 2], [9, 4, 1], [10, 11, 7], [23, 6, 0], [15, 12], [22, 5], [21], [20], [19], [16], [17]]
G_BUCKET_ID_list length 12
backpack scheduling spend  0.12318539619445801
len(g_bucket_nids_list)  12
len(local_split_batches_nid_list)  13
current group_mem  2.802429055094217
current group_mem  2.80263728300465
current group_mem  1.3379273555544262
current group_mem  2.8007725725464603
current group_mem  2.7972885987579055
current group_mem  2.7477890907641793
current group_mem  2.6876900115433844
current group_mem  2.1946883605045633
current group_mem  1.920127067184496
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
batches output list generation spend  0.0006151199340820312
self.weights_list  [0.07998127902895137, 0.08062226879855117, 0.07840424070692015, 0.08050526272949723, 0.07925380651265955, 0.07801252473660916, 0.07785482090440604, 0.07477196534585467, 0.07474144202349278, 0.0746702209379817, 0.07458882541168331, 0.07483301199057847, 0.0717603308728144]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.010731220245361328
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.12395024299621582
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  2.6233954429626465
self.buckets_partition() spend  sec:  0.134721040725708
input layer
dataloader gen time  16.02524161338806
weights_list [0.07998127902895137, 0.08062226879855117, 0.07840424070692015, 0.08050526272949723, 0.07925380651265955, 0.07801252473660916, 0.07785482090440604, 0.07477196534585467, 0.07474144202349278, 0.0746702209379817, 0.07458882541168331, 0.07483301199057847, 0.0717603308728144]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
pure train time  2.2893764972686768
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 23.70263671875 GB
    Memory Allocated: 0.5783395767211914  GigaBytes
Max Memory Allocated: 19.948323249816895  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.3877595663070679
epoch  7
load pickle file time  0.5182323455810547
the output layer 
self.num_batch (get_in_degree_bucketing) 13
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  13
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1599
G_BUCKET_ID_list [[18, 8, 3], [14, 13, 2], [9, 4, 1], [10, 11, 7], [23, 6, 0], [15, 12], [22, 5], [21], [20], [19], [16], [17]]
G_BUCKET_ID_list length 12
backpack scheduling spend  0.302032470703125
len(g_bucket_nids_list)  12
len(local_split_batches_nid_list)  13
current group_mem  2.802429055094217
current group_mem  2.80263728300465
current group_mem  1.3379273555544262
current group_mem  2.8007725725464603
current group_mem  2.7972885987579055
current group_mem  2.7477890907641793
current group_mem  2.6876900115433844
current group_mem  2.1946883605045633
current group_mem  1.920127067184496
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
batches output list generation spend  0.0007765293121337891
self.weights_list  [0.07998127902895137, 0.08062226879855117, 0.07840424070692015, 0.08050526272949723, 0.07925380651265955, 0.07801252473660916, 0.07785482090440604, 0.07477196534585467, 0.07474144202349278, 0.0746702209379817, 0.07458882541168331, 0.07483301199057847, 0.0717603308728144]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.015129566192626953
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.3030738830566406
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  2.6982080936431885
self.buckets_partition() spend  sec:  0.31824755668640137
input layer
dataloader gen time  15.680342435836792
weights_list [0.07998127902895137, 0.08062226879855117, 0.07840424070692015, 0.08050526272949723, 0.07925380651265955, 0.07801252473660916, 0.07785482090440604, 0.07477196534585467, 0.07474144202349278, 0.0746702209379817, 0.07458882541168331, 0.07483301199057847, 0.0717603308728144]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
pure train time  2.2402193546295166
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 23.70263671875 GB
    Memory Allocated: 0.5725846290588379  GigaBytes
Max Memory Allocated: 19.948323249816895  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.2667596340179443
epoch  8
load pickle file time  0.5435166358947754
the output layer 
self.num_batch (get_in_degree_bucketing) 13
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  13
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1599
G_BUCKET_ID_list [[18, 8, 3], [14, 13, 2], [9, 4, 1], [10, 11, 7], [23, 6, 0], [15, 12], [22, 5], [21], [20], [19], [16], [17]]
G_BUCKET_ID_list length 12
backpack scheduling spend  0.11827468872070312
len(g_bucket_nids_list)  12
len(local_split_batches_nid_list)  13
current group_mem  2.802429055094217
current group_mem  2.80263728300465
current group_mem  1.3379273555544262
current group_mem  2.8007725725464603
current group_mem  2.7972885987579055
current group_mem  2.7477890907641793
current group_mem  2.6876900115433844
current group_mem  2.1946883605045633
current group_mem  1.920127067184496
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
batches output list generation spend  0.0005509853363037109
self.weights_list  [0.07998127902895137, 0.08062226879855117, 0.07840424070692015, 0.08050526272949723, 0.07925380651265955, 0.07801252473660916, 0.07785482090440604, 0.07477196534585467, 0.07474144202349278, 0.0746702209379817, 0.07458882541168331, 0.07483301199057847, 0.0717603308728144]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.011734485626220703
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11901044845581055
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  2.495039701461792
self.buckets_partition() spend  sec:  0.1307687759399414
input layer
dataloader gen time  15.583895683288574
weights_list [0.07998127902895137, 0.08062226879855117, 0.07840424070692015, 0.08050526272949723, 0.07925380651265955, 0.07801252473660916, 0.07785482090440604, 0.07477196534585467, 0.07474144202349278, 0.0746702209379817, 0.07458882541168331, 0.07483301199057847, 0.0717603308728144]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
pure train time  2.2567496299743652
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 23.70263671875 GB
    Memory Allocated: 0.5734772682189941  GigaBytes
Max Memory Allocated: 19.948323249816895  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.1745672225952148
epoch  9
load pickle file time  0.45343899726867676
the output layer 
self.num_batch (get_in_degree_bucketing) 13
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  13
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1599
G_BUCKET_ID_list [[18, 8, 3], [14, 13, 2], [9, 4, 1], [10, 11, 7], [23, 6, 0], [15, 12], [22, 5], [21], [20], [19], [16], [17]]
G_BUCKET_ID_list length 12
backpack scheduling spend  0.11670255661010742
len(g_bucket_nids_list)  12
len(local_split_batches_nid_list)  13
current group_mem  2.802429055094217
current group_mem  2.80263728300465
current group_mem  1.3379273555544262
current group_mem  2.8007725725464603
current group_mem  2.7972885987579055
current group_mem  2.7477890907641793
current group_mem  2.6876900115433844
current group_mem  2.1946883605045633
current group_mem  1.920127067184496
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
batches output list generation spend  0.0007207393646240234
self.weights_list  [0.07998127902895137, 0.08062226879855117, 0.07840424070692015, 0.08050526272949723, 0.07925380651265955, 0.07801252473660916, 0.07785482090440604, 0.07477196534585467, 0.07474144202349278, 0.0746702209379817, 0.07458882541168331, 0.07483301199057847, 0.0717603308728144]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.010673046112060547
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11776924133300781
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  2.461792469024658
self.buckets_partition() spend  sec:  0.12846112251281738
input layer
dataloader gen time  14.792559146881104
weights_list [0.07998127902895137, 0.08062226879855117, 0.07840424070692015, 0.08050526272949723, 0.07925380651265955, 0.07801252473660916, 0.07785482090440604, 0.07477196534585467, 0.07474144202349278, 0.0746702209379817, 0.07458882541168331, 0.07483301199057847, 0.0717603308728144]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
pure train time  2.2605040073394775
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 23.70263671875 GB
    Memory Allocated: 0.5752162933349609  GigaBytes
Max Memory Allocated: 19.948323249816895  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.0807242393493652
epoch  10
load pickle file time  0.5176284313201904
the output layer 
self.num_batch (get_in_degree_bucketing) 13
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  13
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1599
G_BUCKET_ID_list [[18, 8, 3], [14, 13, 2], [9, 4, 1], [10, 11, 7], [23, 6, 0], [15, 12], [22, 5], [21], [20], [19], [16], [17]]
G_BUCKET_ID_list length 12
backpack scheduling spend  0.1169588565826416
len(g_bucket_nids_list)  12
len(local_split_batches_nid_list)  13
current group_mem  2.802429055094217
current group_mem  2.80263728300465
current group_mem  1.3379273555544262
current group_mem  2.8007725725464603
current group_mem  2.7972885987579055
current group_mem  2.7477890907641793
current group_mem  2.6876900115433844
current group_mem  2.1946883605045633
current group_mem  1.920127067184496
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
batches output list generation spend  0.0004532337188720703
self.weights_list  [0.07998127902895137, 0.08062226879855117, 0.07840424070692015, 0.08050526272949723, 0.07925380651265955, 0.07801252473660916, 0.07785482090440604, 0.07477196534585467, 0.07474144202349278, 0.0746702209379817, 0.07458882541168331, 0.07483301199057847, 0.0717603308728144]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.010728120803833008
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11754918098449707
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  2.4502127170562744
self.buckets_partition() spend  sec:  0.12829828262329102
input layer
dataloader gen time  14.701034784317017
weights_list [0.07998127902895137, 0.08062226879855117, 0.07840424070692015, 0.08050526272949723, 0.07925380651265955, 0.07801252473660916, 0.07785482090440604, 0.07477196534585467, 0.07474144202349278, 0.0746702209379817, 0.07458882541168331, 0.07483301199057847, 0.0717603308728144]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
pure train time  2.2480356693267822
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 23.70263671875 GB
    Memory Allocated: 0.5740633010864258  GigaBytes
Max Memory Allocated: 19.948323249816895  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.0018185377120972
epoch  11
load pickle file time  0.4821815490722656
the output layer 
self.num_batch (get_in_degree_bucketing) 13
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  13
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1599
G_BUCKET_ID_list [[18, 8, 3], [14, 13, 2], [9, 4, 1], [10, 11, 7], [23, 6, 0], [15, 12], [22, 5], [21], [20], [19], [16], [17]]
G_BUCKET_ID_list length 12
backpack scheduling spend  0.12058830261230469
len(g_bucket_nids_list)  12
len(local_split_batches_nid_list)  13
current group_mem  2.802429055094217
current group_mem  2.80263728300465
current group_mem  1.3379273555544262
current group_mem  2.8007725725464603
current group_mem  2.7972885987579055
current group_mem  2.7477890907641793
current group_mem  2.6876900115433844
current group_mem  2.1946883605045633
current group_mem  1.920127067184496
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
batches output list generation spend  0.0007524490356445312
self.weights_list  [0.07998127902895137, 0.08062226879855117, 0.07840424070692015, 0.08050526272949723, 0.07925380651265955, 0.07801252473660916, 0.07785482090440604, 0.07477196534585467, 0.07474144202349278, 0.0746702209379817, 0.07458882541168331, 0.07483301199057847, 0.0717603308728144]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.011777400970458984
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.12150359153747559
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  2.4594082832336426
self.buckets_partition() spend  sec:  0.13330411911010742
input layer
dataloader gen time  13.488744735717773
weights_list [0.07998127902895137, 0.08062226879855117, 0.07840424070692015, 0.08050526272949723, 0.07925380651265955, 0.07801252473660916, 0.07785482090440604, 0.07477196534585467, 0.07474144202349278, 0.0746702209379817, 0.07458882541168331, 0.07483301199057847, 0.0717603308728144]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
pure train time  2.2444562911987305
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 23.70263671875 GB
    Memory Allocated: 0.5734925270080566  GigaBytes
Max Memory Allocated: 19.948323249816895  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.9573142528533936
epoch  12
load pickle file time  0.5251820087432861
the output layer 
self.num_batch (get_in_degree_bucketing) 13
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  13
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1599
G_BUCKET_ID_list [[18, 8, 3], [14, 13, 2], [9, 4, 1], [10, 11, 7], [23, 6, 0], [15, 12], [22, 5], [21], [20], [19], [16], [17]]
G_BUCKET_ID_list length 12
backpack scheduling spend  0.1173405647277832
len(g_bucket_nids_list)  12
len(local_split_batches_nid_list)  13
current group_mem  2.802429055094217
current group_mem  2.80263728300465
current group_mem  1.3379273555544262
current group_mem  2.8007725725464603
current group_mem  2.7972885987579055
current group_mem  2.7477890907641793
current group_mem  2.6876900115433844
current group_mem  2.1946883605045633
current group_mem  1.920127067184496
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
batches output list generation spend  0.0006453990936279297
self.weights_list  [0.07998127902895137, 0.08062226879855117, 0.07840424070692015, 0.08050526272949723, 0.07925380651265955, 0.07801252473660916, 0.07785482090440604, 0.07477196534585467, 0.07474144202349278, 0.0746702209379817, 0.07458882541168331, 0.07483301199057847, 0.0717603308728144]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.01070261001586914
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11811423301696777
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  2.4534525871276855
self.buckets_partition() spend  sec:  0.12883782386779785
input layer
dataloader gen time  13.604941606521606
weights_list [0.07998127902895137, 0.08062226879855117, 0.07840424070692015, 0.08050526272949723, 0.07925380651265955, 0.07801252473660916, 0.07785482090440604, 0.07477196534585467, 0.07474144202349278, 0.0746702209379817, 0.07458882541168331, 0.07483301199057847, 0.0717603308728144]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
pure train time  2.238828659057617
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 23.70263671875 GB
    Memory Allocated: 0.5751962661743164  GigaBytes
Max Memory Allocated: 19.948323249816895  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.9124066829681396
epoch  13
load pickle file time  0.4729163646697998
the output layer 
self.num_batch (get_in_degree_bucketing) 13
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  13
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1599
G_BUCKET_ID_list [[18, 8, 3], [14, 13, 2], [9, 4, 1], [10, 11, 7], [23, 6, 0], [15, 12], [22, 5], [21], [20], [19], [16], [17]]
G_BUCKET_ID_list length 12
backpack scheduling spend  0.11675262451171875
len(g_bucket_nids_list)  12
len(local_split_batches_nid_list)  13
current group_mem  2.802429055094217
current group_mem  2.80263728300465
current group_mem  1.3379273555544262
current group_mem  2.8007725725464603
current group_mem  2.7972885987579055
current group_mem  2.7477890907641793
current group_mem  2.6876900115433844
current group_mem  2.1946883605045633
current group_mem  1.920127067184496
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
batches output list generation spend  0.0004036426544189453
self.weights_list  [0.07998127902895137, 0.08062226879855117, 0.07840424070692015, 0.08050526272949723, 0.07925380651265955, 0.07801252473660916, 0.07785482090440604, 0.07477196534585467, 0.07474144202349278, 0.0746702209379817, 0.07458882541168331, 0.07483301199057847, 0.0717603308728144]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.010082006454467773
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11728835105895996
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  2.449688673019409
self.buckets_partition() spend  sec:  0.12738919258117676
input layer
dataloader gen time  13.467396974563599
weights_list [0.07998127902895137, 0.08062226879855117, 0.07840424070692015, 0.08050526272949723, 0.07925380651265955, 0.07801252473660916, 0.07785482090440604, 0.07477196534585467, 0.07474144202349278, 0.0746702209379817, 0.07458882541168331, 0.07483301199057847, 0.0717603308728144]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
pure train time  2.25285267829895
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 23.70263671875 GB
    Memory Allocated: 0.5746045112609863  GigaBytes
Max Memory Allocated: 19.948323249816895  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.878318727016449
epoch  14
load pickle file time  0.43129777908325195
the output layer 
self.num_batch (get_in_degree_bucketing) 13
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  13
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1599
G_BUCKET_ID_list [[18, 8, 3], [14, 13, 2], [9, 4, 1], [10, 11, 7], [23, 6, 0], [15, 12], [22, 5], [21], [20], [19], [16], [17]]
G_BUCKET_ID_list length 12
backpack scheduling spend  0.11412358283996582
len(g_bucket_nids_list)  12
len(local_split_batches_nid_list)  13
current group_mem  2.802429055094217
current group_mem  2.80263728300465
current group_mem  1.3379273555544262
current group_mem  2.8007725725464603
current group_mem  2.7972885987579055
current group_mem  2.7477890907641793
current group_mem  2.6876900115433844
current group_mem  2.1946883605045633
current group_mem  1.920127067184496
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
batches output list generation spend  0.0007092952728271484
self.weights_list  [0.07998127902895137, 0.08062226879855117, 0.07840424070692015, 0.08050526272949723, 0.07925380651265955, 0.07801252473660916, 0.07785482090440604, 0.07477196534585467, 0.07474144202349278, 0.0746702209379817, 0.07458882541168331, 0.07483301199057847, 0.0717603308728144]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.009575128555297852
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11501002311706543
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  2.4550914764404297
self.buckets_partition() spend  sec:  0.12461996078491211
input layer
dataloader gen time  13.150019645690918
weights_list [0.07998127902895137, 0.08062226879855117, 0.07840424070692015, 0.08050526272949723, 0.07925380651265955, 0.07801252473660916, 0.07785482090440604, 0.07477196534585467, 0.07474144202349278, 0.0746702209379817, 0.07458882541168331, 0.07483301199057847, 0.0717603308728144]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
pure train time  2.252951145172119
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 23.70263671875 GB
    Memory Allocated: 0.5740561485290527  GigaBytes
Max Memory Allocated: 19.948323249816895  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.8310752511024475
epoch  15
load pickle file time  0.5676579475402832
the output layer 
self.num_batch (get_in_degree_bucketing) 13
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  13
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1599
G_BUCKET_ID_list [[18, 8, 3], [14, 13, 2], [9, 4, 1], [10, 11, 7], [23, 6, 0], [15, 12], [22, 5], [21], [20], [19], [16], [17]]
G_BUCKET_ID_list length 12
backpack scheduling spend  0.11449289321899414
len(g_bucket_nids_list)  12
len(local_split_batches_nid_list)  13
current group_mem  2.802429055094217
current group_mem  2.80263728300465
current group_mem  1.3379273555544262
current group_mem  2.8007725725464603
current group_mem  2.7972885987579055
current group_mem  2.7477890907641793
current group_mem  2.6876900115433844
current group_mem  2.1946883605045633
current group_mem  1.920127067184496
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
batches output list generation spend  0.0005819797515869141
self.weights_list  [0.07998127902895137, 0.08062226879855117, 0.07840424070692015, 0.08050526272949723, 0.07925380651265955, 0.07801252473660916, 0.07785482090440604, 0.07477196534585467, 0.07474144202349278, 0.0746702209379817, 0.07458882541168331, 0.07483301199057847, 0.0717603308728144]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.009284257888793945
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11519765853881836
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  2.460062026977539
self.buckets_partition() spend  sec:  0.12450075149536133
input layer
dataloader gen time  13.790619134902954
weights_list [0.07998127902895137, 0.08062226879855117, 0.07840424070692015, 0.08050526272949723, 0.07925380651265955, 0.07801252473660916, 0.07785482090440604, 0.07477196534585467, 0.07474144202349278, 0.0746702209379817, 0.07458882541168331, 0.07483301199057847, 0.0717603308728144]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
pure train time  2.248744487762451
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 23.70263671875 GB
    Memory Allocated: 0.572899341583252  GigaBytes
Max Memory Allocated: 19.948323249816895  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.7968812584877014
epoch  16
load pickle file time  0.4437687397003174
the output layer 
self.num_batch (get_in_degree_bucketing) 13
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  13
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1599
G_BUCKET_ID_list [[18, 8, 3], [14, 13, 2], [9, 4, 1], [10, 11, 7], [23, 6, 0], [15, 12], [22, 5], [21], [20], [19], [16], [17]]
G_BUCKET_ID_list length 12
backpack scheduling spend  0.11623072624206543
len(g_bucket_nids_list)  12
len(local_split_batches_nid_list)  13
current group_mem  2.802429055094217
current group_mem  2.80263728300465
current group_mem  1.3379273555544262
current group_mem  2.8007725725464603
current group_mem  2.7972885987579055
current group_mem  2.7477890907641793
current group_mem  2.6876900115433844
current group_mem  2.1946883605045633
current group_mem  1.920127067184496
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
batches output list generation spend  0.00042629241943359375
self.weights_list  [0.07998127902895137, 0.08062226879855117, 0.07840424070692015, 0.08050526272949723, 0.07925380651265955, 0.07801252473660916, 0.07785482090440604, 0.07477196534585467, 0.07474144202349278, 0.0746702209379817, 0.07458882541168331, 0.07483301199057847, 0.0717603308728144]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.010839462280273438
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11678385734558105
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  2.45859694480896
self.buckets_partition() spend  sec:  0.12764191627502441
input layer
dataloader gen time  13.119319915771484
weights_list [0.07998127902895137, 0.08062226879855117, 0.07840424070692015, 0.08050526272949723, 0.07925380651265955, 0.07801252473660916, 0.07785482090440604, 0.07477196534585467, 0.07474144202349278, 0.0746702209379817, 0.07458882541168331, 0.07483301199057847, 0.0717603308728144]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
pure train time  2.2492597103118896
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 23.70263671875 GB
    Memory Allocated: 0.5759906768798828  GigaBytes
Max Memory Allocated: 19.948323249816895  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.7606229782104492
epoch  17
load pickle file time  0.4631156921386719
the output layer 
self.num_batch (get_in_degree_bucketing) 13
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  13
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1599
G_BUCKET_ID_list [[18, 8, 3], [14, 13, 2], [9, 4, 1], [10, 11, 7], [23, 6, 0], [15, 12], [22, 5], [21], [20], [19], [16], [17]]
G_BUCKET_ID_list length 12
backpack scheduling spend  0.10537195205688477
len(g_bucket_nids_list)  12
len(local_split_batches_nid_list)  13
current group_mem  2.802429055094217
current group_mem  2.80263728300465
current group_mem  1.3379273555544262
current group_mem  2.8007725725464603
current group_mem  2.7972885987579055
current group_mem  2.7477890907641793
current group_mem  2.6876900115433844
current group_mem  2.1946883605045633
current group_mem  1.920127067184496
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
batches output list generation spend  0.001032114028930664
self.weights_list  [0.07998127902895137, 0.08062226879855117, 0.07840424070692015, 0.08050526272949723, 0.07925380651265955, 0.07801252473660916, 0.07785482090440604, 0.07477196534585467, 0.07474144202349278, 0.0746702209379817, 0.07458882541168331, 0.07483301199057847, 0.0717603308728144]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.00956869125366211
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.10656404495239258
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  2.441084623336792
self.buckets_partition() spend  sec:  0.11615586280822754
input layer
dataloader gen time  13.032068252563477
weights_list [0.07998127902895137, 0.08062226879855117, 0.07840424070692015, 0.08050526272949723, 0.07925380651265955, 0.07801252473660916, 0.07785482090440604, 0.07477196534585467, 0.07474144202349278, 0.0746702209379817, 0.07458882541168331, 0.07483301199057847, 0.0717603308728144]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
pure train time  2.2453017234802246
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 23.70263671875 GB
    Memory Allocated: 0.575648307800293  GigaBytes
Max Memory Allocated: 19.948323249816895  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.7283632159233093
epoch  18
load pickle file time  0.5325973033905029
the output layer 
self.num_batch (get_in_degree_bucketing) 13
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  13
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1599
G_BUCKET_ID_list [[18, 8, 3], [14, 13, 2], [9, 4, 1], [10, 11, 7], [23, 6, 0], [15, 12], [22, 5], [21], [20], [19], [16], [17]]
G_BUCKET_ID_list length 12
backpack scheduling spend  0.11558413505554199
len(g_bucket_nids_list)  12
len(local_split_batches_nid_list)  13
current group_mem  2.802429055094217
current group_mem  2.80263728300465
current group_mem  1.3379273555544262
current group_mem  2.8007725725464603
current group_mem  2.7972885987579055
current group_mem  2.7477890907641793
current group_mem  2.6876900115433844
current group_mem  2.1946883605045633
current group_mem  1.920127067184496
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
batches output list generation spend  0.0004303455352783203
self.weights_list  [0.07998127902895137, 0.08062226879855117, 0.07840424070692015, 0.08050526272949723, 0.07925380651265955, 0.07801252473660916, 0.07785482090440604, 0.07477196534585467, 0.07474144202349278, 0.0746702209379817, 0.07458882541168331, 0.07483301199057847, 0.0717603308728144]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.010866403579711914
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11623334884643555
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  2.4749581813812256
self.buckets_partition() spend  sec:  0.12711882591247559
input layer
dataloader gen time  13.202127695083618
weights_list [0.07998127902895137, 0.08062226879855117, 0.07840424070692015, 0.08050526272949723, 0.07925380651265955, 0.07801252473660916, 0.07785482090440604, 0.07477196534585467, 0.07474144202349278, 0.0746702209379817, 0.07458882541168331, 0.07483301199057847, 0.0717603308728144]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
pure train time  2.2408246994018555
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 23.70263671875 GB
    Memory Allocated: 0.574923038482666  GigaBytes
Max Memory Allocated: 19.948323249816895  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.7008492350578308
epoch  19
load pickle file time  0.5163218975067139
the output layer 
self.num_batch (get_in_degree_bucketing) 13
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  13
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1599
G_BUCKET_ID_list [[18, 8, 3], [14, 13, 2], [9, 4, 1], [10, 11, 7], [23, 6, 0], [15, 12], [22, 5], [21], [20], [19], [16], [17]]
G_BUCKET_ID_list length 12
backpack scheduling spend  0.11501789093017578
len(g_bucket_nids_list)  12
len(local_split_batches_nid_list)  13
current group_mem  2.802429055094217
current group_mem  2.80263728300465
current group_mem  1.3379273555544262
current group_mem  2.8007725725464603
current group_mem  2.7972885987579055
current group_mem  2.7477890907641793
current group_mem  2.6876900115433844
current group_mem  2.1946883605045633
current group_mem  1.920127067184496
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
batches output list generation spend  0.00041484832763671875
self.weights_list  [0.07998127902895137, 0.08062226879855117, 0.07840424070692015, 0.08050526272949723, 0.07925380651265955, 0.07801252473660916, 0.07785482090440604, 0.07477196534585467, 0.07474144202349278, 0.0746702209379817, 0.07458882541168331, 0.07483301199057847, 0.0717603308728144]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.009224176406860352
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11557960510253906
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  2.456984758377075
self.buckets_partition() spend  sec:  0.12482261657714844
input layer
dataloader gen time  13.39958643913269
weights_list [0.07998127902895137, 0.08062226879855117, 0.07840424070692015, 0.08050526272949723, 0.07925380651265955, 0.07801252473660916, 0.07785482090440604, 0.07477196534585467, 0.07474144202349278, 0.0746702209379817, 0.07458882541168331, 0.07483301199057847, 0.0717603308728144]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
pure train time  2.2388205528259277
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 23.70263671875 GB
    Memory Allocated: 0.5775079727172852  GigaBytes
Max Memory Allocated: 19.948323249816895  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.6732544898986816
Total (block generation + training)time/epoch 20.216690680559942
pure train time/epoch 2.251574397087097
dataloader time  [16.473373413085938, 15.87657880783081, 16.00684952735901, 16.02524161338806, 15.680342435836792, 15.583895683288574, 14.792559146881104, 14.701034784317017, 13.488744735717773, 13.604941606521606, 13.467396974563599, 13.150019645690918, 13.790619134902954, 13.119319915771484, 13.032068252563477, 13.202127695083618, 13.39958643913269]
dataloader time avg per epoch 13.92405049617474

num_input_list  [13221660, 13218213, 13220029, 13223373, 13221998, 13215835, 13224391, 13221823, 13218786, 13215102, 13221763, 13219238, 13218245, 13214785, 13218051, 13212431, 13219423, 13216528, 13220188, 13212079]
