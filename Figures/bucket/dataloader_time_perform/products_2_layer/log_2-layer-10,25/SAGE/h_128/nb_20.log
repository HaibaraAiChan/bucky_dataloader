main start at this time 1712915750.424635
-----------------------------------------before load data 
 Nvidia-smi: 0.3560791015625 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

#nodes: 2449029
#edges: 123718024
#classes: 47
success----------------------------------------
196571
39255
2164782
# Nodes: 2400608
# Edges: 123718024
# Train: 196571
# Val: 39255
# Test: 2164782
# Classes: 47

----------------------------------------start of run function 
 Nvidia-smi: 0.3560791015625 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

labels  tensor([0, 1, 2,  ..., 8, 2, 4])
epoch  0
the output layer 
self.num_batch (get_in_degree_bucketing) 20
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  20
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.10016679763793945
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  20
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.00025153160095214844
self.weights_list  [0.04994124260445335, 0.056218872570216356, 0.050536447390510296, 0.049925980943272406, 0.05343616301489029, 0.05457570038306769, 0.055013201336921516, 0.04962583494004711, 0.049615660499259806, 0.04955461385453602, 0.04947321832823763, 0.04971740490713279, 0.049768277111069285, 0.049773364331462934, 0.04961057327886616, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04663454934858143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.01007843017578125
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.1006917953491211
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  3.6761362552642822
self.buckets_partition() spend  sec:  0.11078715324401855
input layer
dataloader gen time  15.31336498260498
weights_list [0.04994124260445335, 0.056218872570216356, 0.050536447390510296, 0.049925980943272406, 0.05343616301489029, 0.05457570038306769, 0.055013201336921516, 0.04962583494004711, 0.049615660499259806, 0.04955461385453602, 0.04947321832823763, 0.04971740490713279, 0.049768277111069285, 0.049773364331462934, 0.04961057327886616, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04663454934858143]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
pure train time  5.155201196670532
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 16.19677734375 GB
    Memory Allocated: 0.48334169387817383  GigaBytes
Max Memory Allocated: 14.05202054977417  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 5.201474189758301
epoch  1
the output layer 
self.num_batch (get_in_degree_bucketing) 20
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  20
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.11667537689208984
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  20
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0004286766052246094
self.weights_list  [0.04994124260445335, 0.056218872570216356, 0.050536447390510296, 0.049925980943272406, 0.05343616301489029, 0.05457570038306769, 0.055013201336921516, 0.04962583494004711, 0.049615660499259806, 0.04955461385453602, 0.04947321832823763, 0.04971740490713279, 0.049768277111069285, 0.049773364331462934, 0.04961057327886616, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04663454934858143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.011720895767211914
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11737394332885742
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  3.773848295211792
self.buckets_partition() spend  sec:  0.12914252281188965
input layer
dataloader gen time  18.695531129837036
weights_list [0.04994124260445335, 0.056218872570216356, 0.050536447390510296, 0.049925980943272406, 0.05343616301489029, 0.05457570038306769, 0.055013201336921516, 0.04962583494004711, 0.049615660499259806, 0.04955461385453602, 0.04947321832823763, 0.04971740490713279, 0.049768277111069285, 0.049773364331462934, 0.04961057327886616, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04663454934858143]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
pure train time  2.6969258785247803
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 16.94482421875 GB
    Memory Allocated: 0.48334503173828125  GigaBytes
Max Memory Allocated: 14.076117992401123  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 3.4748053550720215
epoch  2
the output layer 
self.num_batch (get_in_degree_bucketing) 20
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  20
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.10322332382202148
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  20
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0009748935699462891
self.weights_list  [0.04994124260445335, 0.056218872570216356, 0.050536447390510296, 0.049925980943272406, 0.05343616301489029, 0.05457570038306769, 0.055013201336921516, 0.04962583494004711, 0.049615660499259806, 0.04955461385453602, 0.04947321832823763, 0.04971740490713279, 0.049768277111069285, 0.049773364331462934, 0.04961057327886616, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04663454934858143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.016996383666992188
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.10468459129333496
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  3.7209298610687256
self.buckets_partition() spend  sec:  0.12171101570129395
input layer
dataloader gen time  16.258728504180908
weights_list [0.04994124260445335, 0.056218872570216356, 0.050536447390510296, 0.049925980943272406, 0.05343616301489029, 0.05457570038306769, 0.055013201336921516, 0.04962583494004711, 0.049615660499259806, 0.04955461385453602, 0.04947321832823763, 0.04971740490713279, 0.049768277111069285, 0.049773364331462934, 0.04961057327886616, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04663454934858143]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
pure train time  2.677863597869873
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 16.94482421875 GB
    Memory Allocated: 0.48491525650024414  GigaBytes
Max Memory Allocated: 14.076117992401123  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 2.734105110168457
epoch  3
load pickle file time  0.5437874794006348
the output layer 
self.num_batch (get_in_degree_bucketing) 20
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  20
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.11232399940490723
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  20
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0009849071502685547
self.weights_list  [0.04994124260445335, 0.056218872570216356, 0.050536447390510296, 0.049925980943272406, 0.05343616301489029, 0.05457570038306769, 0.055013201336921516, 0.04962583494004711, 0.049615660499259806, 0.04955461385453602, 0.04947321832823763, 0.04971740490713279, 0.049768277111069285, 0.049773364331462934, 0.04961057327886616, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04663454934858143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.012472867965698242
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11348271369934082
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  3.7059578895568848
self.buckets_partition() spend  sec:  0.1259770393371582
input layer
dataloader gen time  15.002585411071777
weights_list [0.04994124260445335, 0.056218872570216356, 0.050536447390510296, 0.049925980943272406, 0.05343616301489029, 0.05457570038306769, 0.055013201336921516, 0.04962583494004711, 0.049615660499259806, 0.04955461385453602, 0.04947321832823763, 0.04971740490713279, 0.049768277111069285, 0.049773364331462934, 0.04961057327886616, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04663454934858143]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
pure train time  2.665231466293335
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 16.94873046875 GB
    Memory Allocated: 0.48400354385375977  GigaBytes
Max Memory Allocated: 14.109699726104736  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 2.230358362197876
epoch  4
load pickle file time  0.5539050102233887
the output layer 
self.num_batch (get_in_degree_bucketing) 20
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  20
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.11317873001098633
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  20
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.00075531005859375
self.weights_list  [0.04994124260445335, 0.056218872570216356, 0.050536447390510296, 0.049925980943272406, 0.05343616301489029, 0.05457570038306769, 0.055013201336921516, 0.04962583494004711, 0.049615660499259806, 0.04955461385453602, 0.04947321832823763, 0.04971740490713279, 0.049768277111069285, 0.049773364331462934, 0.04961057327886616, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04663454934858143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.014612436294555664
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11408019065856934
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  3.695124864578247
self.buckets_partition() spend  sec:  0.12871241569519043
input layer
dataloader gen time  14.784672260284424
weights_list [0.04994124260445335, 0.056218872570216356, 0.050536447390510296, 0.049925980943272406, 0.05343616301489029, 0.05457570038306769, 0.055013201336921516, 0.04962583494004711, 0.049615660499259806, 0.04955461385453602, 0.04947321832823763, 0.04971740490713279, 0.049768277111069285, 0.049773364331462934, 0.04961057327886616, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04663454934858143]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
pure train time  2.6791317462921143
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 16.96044921875 GB
    Memory Allocated: 0.4846954345703125  GigaBytes
Max Memory Allocated: 14.109699726104736  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.784163475036621
epoch  5
load pickle file time  0.42635488510131836
the output layer 
self.num_batch (get_in_degree_bucketing) 20
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  20
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.10264158248901367
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  20
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0005733966827392578
self.weights_list  [0.04994124260445335, 0.056218872570216356, 0.050536447390510296, 0.049925980943272406, 0.05343616301489029, 0.05457570038306769, 0.055013201336921516, 0.04962583494004711, 0.049615660499259806, 0.04955461385453602, 0.04947321832823763, 0.04971740490713279, 0.049768277111069285, 0.049773364331462934, 0.04961057327886616, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04663454934858143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.013715982437133789
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.1035771369934082
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  3.6890318393707275
self.buckets_partition() spend  sec:  0.11733555793762207
input layer
dataloader gen time  14.240201950073242
weights_list [0.04994124260445335, 0.056218872570216356, 0.050536447390510296, 0.049925980943272406, 0.05343616301489029, 0.05457570038306769, 0.055013201336921516, 0.04962583494004711, 0.049615660499259806, 0.04955461385453602, 0.04947321832823763, 0.04971740490713279, 0.049768277111069285, 0.049773364331462934, 0.04961057327886616, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04663454934858143]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
pure train time  2.6688644886016846
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 16.96044921875 GB
    Memory Allocated: 0.48514556884765625  GigaBytes
Max Memory Allocated: 14.109699726104736  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5526260137557983
epoch  6
load pickle file time  0.5346860885620117
the output layer 
self.num_batch (get_in_degree_bucketing) 20
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  20
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.11206579208374023
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  20
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.000530242919921875
self.weights_list  [0.04994124260445335, 0.056218872570216356, 0.050536447390510296, 0.049925980943272406, 0.05343616301489029, 0.05457570038306769, 0.055013201336921516, 0.04962583494004711, 0.049615660499259806, 0.04955461385453602, 0.04947321832823763, 0.04971740490713279, 0.049768277111069285, 0.049773364331462934, 0.04961057327886616, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04663454934858143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.013492107391357422
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11274290084838867
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  3.7022666931152344
self.buckets_partition() spend  sec:  0.12625432014465332
input layer
dataloader gen time  14.499200344085693
weights_list [0.04994124260445335, 0.056218872570216356, 0.050536447390510296, 0.049925980943272406, 0.05343616301489029, 0.05457570038306769, 0.055013201336921516, 0.04962583494004711, 0.049615660499259806, 0.04955461385453602, 0.04947321832823763, 0.04971740490713279, 0.049768277111069285, 0.049773364331462934, 0.04961057327886616, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04663454934858143]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
pure train time  2.6753270626068115
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 16.96044921875 GB
    Memory Allocated: 0.4846944808959961  GigaBytes
Max Memory Allocated: 14.109699726104736  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.3956035375595093
epoch  7
load pickle file time  0.48120713233947754
the output layer 
self.num_batch (get_in_degree_bucketing) 20
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  20
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.3625917434692383
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  20
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0005307197570800781
self.weights_list  [0.04994124260445335, 0.056218872570216356, 0.050536447390510296, 0.049925980943272406, 0.05343616301489029, 0.05457570038306769, 0.055013201336921516, 0.04962583494004711, 0.049615660499259806, 0.04955461385453602, 0.04947321832823763, 0.04971740490713279, 0.049768277111069285, 0.049773364331462934, 0.04961057327886616, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04663454934858143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.011316776275634766
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.3632774353027344
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  3.976020097732544
self.buckets_partition() spend  sec:  0.3746147155761719
input layer
dataloader gen time  14.714913368225098
weights_list [0.04994124260445335, 0.056218872570216356, 0.050536447390510296, 0.049925980943272406, 0.05343616301489029, 0.05457570038306769, 0.055013201336921516, 0.04962583494004711, 0.049615660499259806, 0.04955461385453602, 0.04947321832823763, 0.04971740490713279, 0.049768277111069285, 0.049773364331462934, 0.04961057327886616, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04663454934858143]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
pure train time  2.678088665008545
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 16.96044921875 GB
    Memory Allocated: 0.48467206954956055  GigaBytes
Max Memory Allocated: 14.109699726104736  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.274754524230957
epoch  8
load pickle file time  0.5096023082733154
the output layer 
self.num_batch (get_in_degree_bucketing) 20
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  20
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.11246538162231445
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  20
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0006003379821777344
self.weights_list  [0.04994124260445335, 0.056218872570216356, 0.050536447390510296, 0.049925980943272406, 0.05343616301489029, 0.05457570038306769, 0.055013201336921516, 0.04962583494004711, 0.049615660499259806, 0.04955461385453602, 0.04947321832823763, 0.04971740490713279, 0.049768277111069285, 0.049773364331462934, 0.04961057327886616, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04663454934858143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.014005422592163086
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11326718330383301
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  3.707899808883667
self.buckets_partition() spend  sec:  0.12729358673095703
input layer
dataloader gen time  14.28846549987793
weights_list [0.04994124260445335, 0.056218872570216356, 0.050536447390510296, 0.049925980943272406, 0.05343616301489029, 0.05457570038306769, 0.055013201336921516, 0.04962583494004711, 0.049615660499259806, 0.04955461385453602, 0.04947321832823763, 0.04971740490713279, 0.049768277111069285, 0.049773364331462934, 0.04961057327886616, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04663454934858143]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
pure train time  2.6547231674194336
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 16.96044921875 GB
    Memory Allocated: 0.48291826248168945  GigaBytes
Max Memory Allocated: 14.109699726104736  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.1753007173538208
epoch  9
load pickle file time  0.5337560176849365
the output layer 
self.num_batch (get_in_degree_bucketing) 20
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  20
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.11368727684020996
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  20
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0004298686981201172
self.weights_list  [0.04994124260445335, 0.056218872570216356, 0.050536447390510296, 0.049925980943272406, 0.05343616301489029, 0.05457570038306769, 0.055013201336921516, 0.04962583494004711, 0.049615660499259806, 0.04955461385453602, 0.04947321832823763, 0.04971740490713279, 0.049768277111069285, 0.049773364331462934, 0.04961057327886616, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04663454934858143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.01328587532043457
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11426472663879395
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  3.730194091796875
self.buckets_partition() spend  sec:  0.12757062911987305
input layer
dataloader gen time  14.09471082687378
weights_list [0.04994124260445335, 0.056218872570216356, 0.050536447390510296, 0.049925980943272406, 0.05343616301489029, 0.05457570038306769, 0.055013201336921516, 0.04962583494004711, 0.049615660499259806, 0.04955461385453602, 0.04947321832823763, 0.04971740490713279, 0.049768277111069285, 0.049773364331462934, 0.04961057327886616, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04663454934858143]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
pure train time  2.641984224319458
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 16.96044921875 GB
    Memory Allocated: 0.4835028648376465  GigaBytes
Max Memory Allocated: 14.109699726104736  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.0795762538909912
epoch  10
load pickle file time  0.5677249431610107
the output layer 
self.num_batch (get_in_degree_bucketing) 20
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  20
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.10300397872924805
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  20
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.00048661231994628906
self.weights_list  [0.04994124260445335, 0.056218872570216356, 0.050536447390510296, 0.049925980943272406, 0.05343616301489029, 0.05457570038306769, 0.055013201336921516, 0.04962583494004711, 0.049615660499259806, 0.04955461385453602, 0.04947321832823763, 0.04971740490713279, 0.049768277111069285, 0.049773364331462934, 0.04961057327886616, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04663454934858143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.010005474090576172
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.1036379337310791
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  3.733205556869507
self.buckets_partition() spend  sec:  0.11366081237792969
input layer
dataloader gen time  14.520856142044067
weights_list [0.04994124260445335, 0.056218872570216356, 0.050536447390510296, 0.049925980943272406, 0.05343616301489029, 0.05457570038306769, 0.055013201336921516, 0.04962583494004711, 0.049615660499259806, 0.04955461385453602, 0.04947321832823763, 0.04971740490713279, 0.049768277111069285, 0.049773364331462934, 0.04961057327886616, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04663454934858143]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
pure train time  2.670132875442505
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 16.96044921875 GB
    Memory Allocated: 0.48340940475463867  GigaBytes
Max Memory Allocated: 14.109699726104736  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.000813364982605
epoch  11
load pickle file time  0.5494334697723389
the output layer 
self.num_batch (get_in_degree_bucketing) 20
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  20
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.11245059967041016
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  20
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0003936290740966797
self.weights_list  [0.04994124260445335, 0.056218872570216356, 0.050536447390510296, 0.049925980943272406, 0.05343616301489029, 0.05457570038306769, 0.055013201336921516, 0.04962583494004711, 0.049615660499259806, 0.04955461385453602, 0.04947321832823763, 0.04971740490713279, 0.049768277111069285, 0.049773364331462934, 0.04961057327886616, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04663454934858143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.010004281997680664
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11300802230834961
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  3.7240002155303955
self.buckets_partition() spend  sec:  0.12303376197814941
input layer
dataloader gen time  13.751318454742432
weights_list [0.04994124260445335, 0.056218872570216356, 0.050536447390510296, 0.049925980943272406, 0.05343616301489029, 0.05457570038306769, 0.055013201336921516, 0.04962583494004711, 0.049615660499259806, 0.04955461385453602, 0.04947321832823763, 0.04971740490713279, 0.049768277111069285, 0.049773364331462934, 0.04961057327886616, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04663454934858143]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
pure train time  2.678128242492676
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 16.96044921875 GB
    Memory Allocated: 0.4842958450317383  GigaBytes
Max Memory Allocated: 14.109699726104736  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.9574233889579773
epoch  12
load pickle file time  0.5276341438293457
the output layer 
self.num_batch (get_in_degree_bucketing) 20
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  20
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.11323928833007812
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  20
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0005972385406494141
self.weights_list  [0.04994124260445335, 0.056218872570216356, 0.050536447390510296, 0.049925980943272406, 0.05343616301489029, 0.05457570038306769, 0.055013201336921516, 0.04962583494004711, 0.049615660499259806, 0.04955461385453602, 0.04947321832823763, 0.04971740490713279, 0.049768277111069285, 0.049773364331462934, 0.04961057327886616, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04663454934858143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.013147115707397461
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11398887634277344
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  3.714592456817627
self.buckets_partition() spend  sec:  0.1271533966064453
input layer
dataloader gen time  13.842706441879272
weights_list [0.04994124260445335, 0.056218872570216356, 0.050536447390510296, 0.049925980943272406, 0.05343616301489029, 0.05457570038306769, 0.055013201336921516, 0.04962583494004711, 0.049615660499259806, 0.04955461385453602, 0.04947321832823763, 0.04971740490713279, 0.049768277111069285, 0.049773364331462934, 0.04961057327886616, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04663454934858143]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
pure train time  2.674673318862915
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 16.96044921875 GB
    Memory Allocated: 0.4845571517944336  GigaBytes
Max Memory Allocated: 14.109699726104736  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.9106796383857727
epoch  13
load pickle file time  0.5507724285125732
the output layer 
self.num_batch (get_in_degree_bucketing) 20
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  20
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.10304880142211914
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  20
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.00048542022705078125
self.weights_list  [0.04994124260445335, 0.056218872570216356, 0.050536447390510296, 0.049925980943272406, 0.05343616301489029, 0.05457570038306769, 0.055013201336921516, 0.04962583494004711, 0.049615660499259806, 0.04955461385453602, 0.04947321832823763, 0.04971740490713279, 0.049768277111069285, 0.049773364331462934, 0.04961057327886616, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04663454934858143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.011185646057128906
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.10370230674743652
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  3.725330352783203
self.buckets_partition() spend  sec:  0.1149137020111084
input layer
dataloader gen time  14.037094354629517
weights_list [0.04994124260445335, 0.056218872570216356, 0.050536447390510296, 0.049925980943272406, 0.05343616301489029, 0.05457570038306769, 0.055013201336921516, 0.04962583494004711, 0.049615660499259806, 0.04955461385453602, 0.04947321832823763, 0.04971740490713279, 0.049768277111069285, 0.049773364331462934, 0.04961057327886616, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04663454934858143]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
pure train time  2.6620934009552
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 16.96044921875 GB
    Memory Allocated: 0.4835505485534668  GigaBytes
Max Memory Allocated: 14.109699726104736  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.8733464479446411
epoch  14
load pickle file time  0.5682404041290283
the output layer 
self.num_batch (get_in_degree_bucketing) 20
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  20
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.11264348030090332
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  20
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0005080699920654297
self.weights_list  [0.04994124260445335, 0.056218872570216356, 0.050536447390510296, 0.049925980943272406, 0.05343616301489029, 0.05457570038306769, 0.055013201336921516, 0.04962583494004711, 0.049615660499259806, 0.04955461385453602, 0.04947321832823763, 0.04971740490713279, 0.049768277111069285, 0.049773364331462934, 0.04961057327886616, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04663454934858143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.010560989379882812
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11329817771911621
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  3.7166213989257812
self.buckets_partition() spend  sec:  0.12387776374816895
input layer
dataloader gen time  14.018902063369751
weights_list [0.04994124260445335, 0.056218872570216356, 0.050536447390510296, 0.049925980943272406, 0.05343616301489029, 0.05457570038306769, 0.055013201336921516, 0.04962583494004711, 0.049615660499259806, 0.04955461385453602, 0.04947321832823763, 0.04971740490713279, 0.049768277111069285, 0.049773364331462934, 0.04961057327886616, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04663454934858143]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
pure train time  2.66762113571167
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 16.96044921875 GB
    Memory Allocated: 0.4836001396179199  GigaBytes
Max Memory Allocated: 14.109699726104736  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.8313072919845581
epoch  15
load pickle file time  0.574169397354126
the output layer 
self.num_batch (get_in_degree_bucketing) 20
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  20
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.1135866641998291
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  20
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0004284381866455078
self.weights_list  [0.04994124260445335, 0.056218872570216356, 0.050536447390510296, 0.049925980943272406, 0.05343616301489029, 0.05457570038306769, 0.055013201336921516, 0.04962583494004711, 0.049615660499259806, 0.04955461385453602, 0.04947321832823763, 0.04971740490713279, 0.049768277111069285, 0.049773364331462934, 0.04961057327886616, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04663454934858143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.009544610977172852
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11424040794372559
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  3.754939317703247
self.buckets_partition() spend  sec:  0.12380290031433105
input layer
dataloader gen time  14.488863706588745
weights_list [0.04994124260445335, 0.056218872570216356, 0.050536447390510296, 0.049925980943272406, 0.05343616301489029, 0.05457570038306769, 0.055013201336921516, 0.04962583494004711, 0.049615660499259806, 0.04955461385453602, 0.04947321832823763, 0.04971740490713279, 0.049768277111069285, 0.049773364331462934, 0.04961057327886616, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04663454934858143]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
pure train time  2.6594955921173096
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 16.96044921875 GB
    Memory Allocated: 0.48297739028930664  GigaBytes
Max Memory Allocated: 14.113749504089355  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.7973322868347168
epoch  16
load pickle file time  0.4919424057006836
the output layer 
self.num_batch (get_in_degree_bucketing) 20
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  20
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.10286831855773926
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  20
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.00038743019104003906
self.weights_list  [0.04994124260445335, 0.056218872570216356, 0.050536447390510296, 0.049925980943272406, 0.05343616301489029, 0.05457570038306769, 0.055013201336921516, 0.04962583494004711, 0.049615660499259806, 0.04955461385453602, 0.04947321832823763, 0.04971740490713279, 0.049768277111069285, 0.049773364331462934, 0.04961057327886616, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04663454934858143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.009615421295166016
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.10341382026672363
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  3.709059715270996
self.buckets_partition() spend  sec:  0.11304926872253418
input layer
dataloader gen time  13.7672700881958
weights_list [0.04994124260445335, 0.056218872570216356, 0.050536447390510296, 0.049925980943272406, 0.05343616301489029, 0.05457570038306769, 0.055013201336921516, 0.04962583494004711, 0.049615660499259806, 0.04955461385453602, 0.04947321832823763, 0.04971740490713279, 0.049768277111069285, 0.049773364331462934, 0.04961057327886616, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04663454934858143]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
pure train time  2.6464524269104004
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 16.96044921875 GB
    Memory Allocated: 0.4833965301513672  GigaBytes
Max Memory Allocated: 14.113749504089355  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.7620957493782043
epoch  17
load pickle file time  0.5455269813537598
the output layer 
self.num_batch (get_in_degree_bucketing) 20
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  20
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.1129150390625
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  20
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0005421638488769531
self.weights_list  [0.04994124260445335, 0.056218872570216356, 0.050536447390510296, 0.049925980943272406, 0.05343616301489029, 0.05457570038306769, 0.055013201336921516, 0.04962583494004711, 0.049615660499259806, 0.04955461385453602, 0.04947321832823763, 0.04971740490713279, 0.049768277111069285, 0.049773364331462934, 0.04961057327886616, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04663454934858143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.010503053665161133
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11361145973205566
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  3.7216238975524902
self.buckets_partition() spend  sec:  0.12413311004638672
input layer
dataloader gen time  13.872912168502808
weights_list [0.04994124260445335, 0.056218872570216356, 0.050536447390510296, 0.049925980943272406, 0.05343616301489029, 0.05457570038306769, 0.055013201336921516, 0.04962583494004711, 0.049615660499259806, 0.04955461385453602, 0.04947321832823763, 0.04971740490713279, 0.049768277111069285, 0.049773364331462934, 0.04961057327886616, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04663454934858143]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
pure train time  2.6954376697540283
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 16.96044921875 GB
    Memory Allocated: 0.4845724105834961  GigaBytes
Max Memory Allocated: 14.113749504089355  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.7295279502868652
epoch  18
load pickle file time  0.5401487350463867
the output layer 
self.num_batch (get_in_degree_bucketing) 20
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  20
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.1140131950378418
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  20
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0004551410675048828
self.weights_list  [0.04994124260445335, 0.056218872570216356, 0.050536447390510296, 0.049925980943272406, 0.05343616301489029, 0.05457570038306769, 0.055013201336921516, 0.04962583494004711, 0.049615660499259806, 0.04955461385453602, 0.04947321832823763, 0.04971740490713279, 0.049768277111069285, 0.049773364331462934, 0.04961057327886616, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04663454934858143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.011053085327148438
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.1147758960723877
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  3.714395761489868
self.buckets_partition() spend  sec:  0.12584853172302246
input layer
dataloader gen time  13.843504428863525
weights_list [0.04994124260445335, 0.056218872570216356, 0.050536447390510296, 0.049925980943272406, 0.05343616301489029, 0.05457570038306769, 0.055013201336921516, 0.04962583494004711, 0.049615660499259806, 0.04955461385453602, 0.04947321832823763, 0.04971740490713279, 0.049768277111069285, 0.049773364331462934, 0.04961057327886616, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04663454934858143]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
pure train time  2.703913450241089
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 16.96044921875 GB
    Memory Allocated: 0.4826645851135254  GigaBytes
Max Memory Allocated: 14.113749504089355  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.7010665535926819
epoch  19
load pickle file time  0.4905104637145996
the output layer 
self.num_batch (get_in_degree_bucketing) 20
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  20
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.10394144058227539
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  20
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0005745887756347656
self.weights_list  [0.04994124260445335, 0.056218872570216356, 0.050536447390510296, 0.049925980943272406, 0.05343616301489029, 0.05457570038306769, 0.055013201336921516, 0.04962583494004711, 0.049615660499259806, 0.04955461385453602, 0.04947321832823763, 0.04971740490713279, 0.049768277111069285, 0.049773364331462934, 0.04961057327886616, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04663454934858143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.010727882385253906
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.10467147827148438
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  3.703691005706787
self.buckets_partition() spend  sec:  0.11541867256164551
input layer
dataloader gen time  13.776658535003662
weights_list [0.04994124260445335, 0.056218872570216356, 0.050536447390510296, 0.049925980943272406, 0.05343616301489029, 0.05457570038306769, 0.055013201336921516, 0.04962583494004711, 0.049615660499259806, 0.04955461385453602, 0.04947321832823763, 0.04971740490713279, 0.049768277111069285, 0.049773364331462934, 0.04961057327886616, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04664472378936873, 0.04663454934858143]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
step  18
step  19
pure train time  2.716519355773926
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 16.96044921875 GB
    Memory Allocated: 0.48377227783203125  GigaBytes
Max Memory Allocated: 14.113749504089355  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.6719560027122498
Total (block generation + training)time/epoch 20.610689036986408
pure train time/epoch 2.6732866764068604
dataloader time  [15.002585411071777, 14.784672260284424, 14.240201950073242, 14.499200344085693, 14.714913368225098, 14.28846549987793, 14.09471082687378, 14.520856142044067, 13.751318454742432, 13.842706441879272, 14.037094354629517, 14.018902063369751, 14.488863706588745, 13.7672700881958, 13.872912168502808, 13.843504428863525, 13.776658535003662]
dataloader time avg per epoch 14.078321236830492

num_input_list  [16929652, 16926308, 16917667, 16930335, 16926576, 16926476, 16932217, 16921534, 16921443, 16922428, 16914773, 16926824, 16925376, 16913891, 16918043, 16915583, 16923029, 16918902, 16924884, 16917231]
