main start at this time 1712914733.738651
-----------------------------------------before load data 
 Nvidia-smi: 0.3560791015625 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

#nodes: 2449029
#edges: 123718024
#classes: 47
success----------------------------------------
196571
39255
2164782
# Nodes: 2400608
# Edges: 123718024
# Train: 196571
# Val: 39255
# Test: 2164782
# Classes: 47

----------------------------------------start of run function 
 Nvidia-smi: 0.3560791015625 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

labels  tensor([0, 1, 2,  ..., 8, 2, 4])
epoch  0
the output layer 
self.num_batch (get_in_degree_bucketing) 18
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  18
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.10990762710571289
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  18
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.00032830238342285156
self.weights_list  [0.0551251201855818, 0.061402750151344804, 0.05572032497163874, 0.055109858524400854, 0.05862004059601874, 0.05975957796419614, 0.060197078918049964, 0.05480971252117556, 0.05479953808038825, 0.05473849143566447, 0.05465709590936608, 0.05490128248826124, 0.05495215469219773, 0.05495724191259138, 0.054794450859994605, 0.051828601370497177, 0.051828601370497177, 0.05179807804813528]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.010501623153686523
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11050057411193848
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  3.3647334575653076
self.buckets_partition() spend  sec:  0.1210167407989502
input layer
dataloader gen time  15.248450994491577
weights_list [0.0551251201855818, 0.061402750151344804, 0.05572032497163874, 0.055109858524400854, 0.05862004059601874, 0.05975957796419614, 0.060197078918049964, 0.05480971252117556, 0.05479953808038825, 0.05473849143566447, 0.05465709590936608, 0.05490128248826124, 0.05495215469219773, 0.05495724191259138, 0.054794450859994605, 0.051828601370497177, 0.051828601370497177, 0.05179807804813528]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
pure train time  5.050929307937622
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 16.75732421875 GB
    Memory Allocated: 0.5218815803527832  GigaBytes
Max Memory Allocated: 15.331234455108643  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 5.206728458404541
epoch  1
the output layer 
self.num_batch (get_in_degree_bucketing) 18
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  18
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.10380768775939941
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  18
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0006191730499267578
self.weights_list  [0.0551251201855818, 0.061402750151344804, 0.05572032497163874, 0.055109858524400854, 0.05862004059601874, 0.05975957796419614, 0.060197078918049964, 0.05480971252117556, 0.05479953808038825, 0.05473849143566447, 0.05465709590936608, 0.05490128248826124, 0.05495215469219773, 0.05495724191259138, 0.054794450859994605, 0.051828601370497177, 0.051828601370497177, 0.05179807804813528]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.01118016242980957
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.10461544990539551
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  3.4514684677124023
self.buckets_partition() spend  sec:  0.11581730842590332
input layer
dataloader gen time  17.201565265655518
weights_list [0.0551251201855818, 0.061402750151344804, 0.05572032497163874, 0.055109858524400854, 0.05862004059601874, 0.05975957796419614, 0.060197078918049964, 0.05480971252117556, 0.05479953808038825, 0.05473849143566447, 0.05465709590936608, 0.05490128248826124, 0.05495215469219773, 0.05495724191259138, 0.054794450859994605, 0.051828601370497177, 0.051828601370497177, 0.05179807804813528]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
pure train time  2.523729085922241
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 16.75732421875 GB
    Memory Allocated: 0.5163936614990234  GigaBytes
Max Memory Allocated: 15.331234455108643  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 3.48056960105896
epoch  2
the output layer 
self.num_batch (get_in_degree_bucketing) 18
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  18
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.12028908729553223
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  18
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.001046895980834961
self.weights_list  [0.0551251201855818, 0.061402750151344804, 0.05572032497163874, 0.055109858524400854, 0.05862004059601874, 0.05975957796419614, 0.060197078918049964, 0.05480971252117556, 0.05479953808038825, 0.05473849143566447, 0.05465709590936608, 0.05490128248826124, 0.05495215469219773, 0.05495724191259138, 0.054794450859994605, 0.051828601370497177, 0.051828601370497177, 0.05179807804813528]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.015610933303833008
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.12182307243347168
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  3.492661714553833
self.buckets_partition() spend  sec:  0.13745808601379395
input layer
dataloader gen time  16.870527744293213
weights_list [0.0551251201855818, 0.061402750151344804, 0.05572032497163874, 0.055109858524400854, 0.05862004059601874, 0.05975957796419614, 0.060197078918049964, 0.05480971252117556, 0.05479953808038825, 0.05473849143566447, 0.05465709590936608, 0.05490128248826124, 0.05495215469219773, 0.05495724191259138, 0.054794450859994605, 0.051828601370497177, 0.051828601370497177, 0.05179807804813528]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
pure train time  2.5656723976135254
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 16.75927734375 GB
    Memory Allocated: 0.5234088897705078  GigaBytes
Max Memory Allocated: 15.331234455108643  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 2.7314672470092773
epoch  3
load pickle file time  0.6168766021728516
the output layer 
self.num_batch (get_in_degree_bucketing) 18
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  18
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.11499309539794922
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  18
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0005314350128173828
self.weights_list  [0.0551251201855818, 0.061402750151344804, 0.05572032497163874, 0.055109858524400854, 0.05862004059601874, 0.05975957796419614, 0.060197078918049964, 0.05480971252117556, 0.05479953808038825, 0.05473849143566447, 0.05465709590936608, 0.05490128248826124, 0.05495215469219773, 0.05495724191259138, 0.054794450859994605, 0.051828601370497177, 0.051828601370497177, 0.05179807804813528]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.012645483016967773
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11572432518005371
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  3.384888172149658
self.buckets_partition() spend  sec:  0.12839293479919434
input layer
dataloader gen time  15.185631513595581
weights_list [0.0551251201855818, 0.061402750151344804, 0.05572032497163874, 0.055109858524400854, 0.05862004059601874, 0.05975957796419614, 0.060197078918049964, 0.05480971252117556, 0.05479953808038825, 0.05473849143566447, 0.05465709590936608, 0.05490128248826124, 0.05495215469219773, 0.05495724191259138, 0.054794450859994605, 0.051828601370497177, 0.051828601370497177, 0.05179807804813528]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
pure train time  2.5077311992645264
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 16.75927734375 GB
    Memory Allocated: 0.5189900398254395  GigaBytes
Max Memory Allocated: 15.331234455108643  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 2.239609956741333
epoch  4
load pickle file time  0.5998036861419678
the output layer 
self.num_batch (get_in_degree_bucketing) 18
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  18
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.11464190483093262
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  18
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0005426406860351562
self.weights_list  [0.0551251201855818, 0.061402750151344804, 0.05572032497163874, 0.055109858524400854, 0.05862004059601874, 0.05975957796419614, 0.060197078918049964, 0.05480971252117556, 0.05479953808038825, 0.05473849143566447, 0.05465709590936608, 0.05490128248826124, 0.05495215469219773, 0.05495724191259138, 0.054794450859994605, 0.051828601370497177, 0.051828601370497177, 0.05179807804813528]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.01365351676940918
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11542105674743652
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  3.3672537803649902
self.buckets_partition() spend  sec:  0.12910699844360352
input layer
dataloader gen time  14.429777383804321
weights_list [0.0551251201855818, 0.061402750151344804, 0.05572032497163874, 0.055109858524400854, 0.05862004059601874, 0.05975957796419614, 0.060197078918049964, 0.05480971252117556, 0.05479953808038825, 0.05473849143566447, 0.05465709590936608, 0.05490128248826124, 0.05495215469219773, 0.05495724191259138, 0.054794450859994605, 0.051828601370497177, 0.051828601370497177, 0.05179807804813528]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
pure train time  2.520937204360962
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 17.55810546875 GB
    Memory Allocated: 0.520477294921875  GigaBytes
Max Memory Allocated: 15.331234455108643  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.780924677848816
epoch  5
load pickle file time  0.490081787109375
the output layer 
self.num_batch (get_in_degree_bucketing) 18
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  18
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.10617733001708984
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  18
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0005872249603271484
self.weights_list  [0.0551251201855818, 0.061402750151344804, 0.05572032497163874, 0.055109858524400854, 0.05862004059601874, 0.05975957796419614, 0.060197078918049964, 0.05480971252117556, 0.05479953808038825, 0.05473849143566447, 0.05465709590936608, 0.05490128248826124, 0.05495215469219773, 0.05495724191259138, 0.054794450859994605, 0.051828601370497177, 0.051828601370497177, 0.05179807804813528]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.013353824615478516
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.10693621635437012
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  3.4574689865112305
self.buckets_partition() spend  sec:  0.12030744552612305
input layer
dataloader gen time  13.99072265625
weights_list [0.0551251201855818, 0.061402750151344804, 0.05572032497163874, 0.055109858524400854, 0.05862004059601874, 0.05975957796419614, 0.060197078918049964, 0.05480971252117556, 0.05479953808038825, 0.05473849143566447, 0.05465709590936608, 0.05490128248826124, 0.05495215469219773, 0.05495724191259138, 0.054794450859994605, 0.051828601370497177, 0.051828601370497177, 0.05179807804813528]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
pure train time  2.5330541133880615
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 17.55810546875 GB
    Memory Allocated: 0.5241489410400391  GigaBytes
Max Memory Allocated: 15.331234455108643  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5529143810272217
epoch  6
load pickle file time  0.5468401908874512
the output layer 
self.num_batch (get_in_degree_bucketing) 18
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  18
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.11378741264343262
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  18
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0005137920379638672
self.weights_list  [0.0551251201855818, 0.061402750151344804, 0.05572032497163874, 0.055109858524400854, 0.05862004059601874, 0.05975957796419614, 0.060197078918049964, 0.05480971252117556, 0.05479953808038825, 0.05473849143566447, 0.05465709590936608, 0.05490128248826124, 0.05495215469219773, 0.05495724191259138, 0.054794450859994605, 0.051828601370497177, 0.051828601370497177, 0.05179807804813528]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.010416269302368164
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11446738243103027
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  3.3870744705200195
self.buckets_partition() spend  sec:  0.12490415573120117
input layer
dataloader gen time  14.331257343292236
weights_list [0.0551251201855818, 0.061402750151344804, 0.05572032497163874, 0.055109858524400854, 0.05862004059601874, 0.05975957796419614, 0.060197078918049964, 0.05480971252117556, 0.05479953808038825, 0.05473849143566447, 0.05465709590936608, 0.05490128248826124, 0.05495215469219773, 0.05495724191259138, 0.054794450859994605, 0.051828601370497177, 0.051828601370497177, 0.05179807804813528]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
pure train time  2.529927968978882
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 17.55810546875 GB
    Memory Allocated: 0.5236148834228516  GigaBytes
Max Memory Allocated: 15.331234455108643  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.3964945077896118
epoch  7
load pickle file time  0.5612533092498779
the output layer 
self.num_batch (get_in_degree_bucketing) 18
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  18
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.11398029327392578
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  18
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0006508827209472656
self.weights_list  [0.0551251201855818, 0.061402750151344804, 0.05572032497163874, 0.055109858524400854, 0.05862004059601874, 0.05975957796419614, 0.060197078918049964, 0.05480971252117556, 0.05479953808038825, 0.05473849143566447, 0.05465709590936608, 0.05490128248826124, 0.05495215469219773, 0.05495724191259138, 0.054794450859994605, 0.051828601370497177, 0.051828601370497177, 0.05179807804813528]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.012498140335083008
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11478161811828613
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  3.4178526401519775
self.buckets_partition() spend  sec:  0.12729787826538086
input layer
dataloader gen time  13.897758483886719
weights_list [0.0551251201855818, 0.061402750151344804, 0.05572032497163874, 0.055109858524400854, 0.05862004059601874, 0.05975957796419614, 0.060197078918049964, 0.05480971252117556, 0.05479953808038825, 0.05473849143566447, 0.05465709590936608, 0.05490128248826124, 0.05495215469219773, 0.05495724191259138, 0.054794450859994605, 0.051828601370497177, 0.051828601370497177, 0.05179807804813528]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
pure train time  2.525658369064331
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 17.55810546875 GB
    Memory Allocated: 0.5192160606384277  GigaBytes
Max Memory Allocated: 15.331234455108643  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.2693625688552856
epoch  8
load pickle file time  0.56197190284729
the output layer 
self.num_batch (get_in_degree_bucketing) 18
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  18
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.1047358512878418
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  18
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0003955364227294922
self.weights_list  [0.0551251201855818, 0.061402750151344804, 0.05572032497163874, 0.055109858524400854, 0.05862004059601874, 0.05975957796419614, 0.060197078918049964, 0.05480971252117556, 0.05479953808038825, 0.05473849143566447, 0.05465709590936608, 0.05490128248826124, 0.05495215469219773, 0.05495724191259138, 0.054794450859994605, 0.051828601370497177, 0.051828601370497177, 0.05179807804813528]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.011240720748901367
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.1052708625793457
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  3.364722967147827
self.buckets_partition() spend  sec:  0.11652898788452148
input layer
dataloader gen time  13.819269180297852
weights_list [0.0551251201855818, 0.061402750151344804, 0.05572032497163874, 0.055109858524400854, 0.05862004059601874, 0.05975957796419614, 0.060197078918049964, 0.05480971252117556, 0.05479953808038825, 0.05473849143566447, 0.05465709590936608, 0.05490128248826124, 0.05495215469219773, 0.05495724191259138, 0.054794450859994605, 0.051828601370497177, 0.051828601370497177, 0.05179807804813528]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
pure train time  2.543009042739868
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 17.55810546875 GB
    Memory Allocated: 0.5208163261413574  GigaBytes
Max Memory Allocated: 15.331234455108643  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.177714467048645
epoch  9
load pickle file time  0.510991096496582
the output layer 
self.num_batch (get_in_degree_bucketing) 18
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  18
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.11359024047851562
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  18
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.00036835670471191406
self.weights_list  [0.0551251201855818, 0.061402750151344804, 0.05572032497163874, 0.055109858524400854, 0.05862004059601874, 0.05975957796419614, 0.060197078918049964, 0.05480971252117556, 0.05479953808038825, 0.05473849143566447, 0.05465709590936608, 0.05490128248826124, 0.05495215469219773, 0.05495724191259138, 0.054794450859994605, 0.051828601370497177, 0.051828601370497177, 0.05179807804813528]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.010250091552734375
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11412262916564941
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  3.407496452331543
self.buckets_partition() spend  sec:  0.12439513206481934
input layer
dataloader gen time  13.254985570907593
weights_list [0.0551251201855818, 0.061402750151344804, 0.05572032497163874, 0.055109858524400854, 0.05862004059601874, 0.05975957796419614, 0.060197078918049964, 0.05480971252117556, 0.05479953808038825, 0.05473849143566447, 0.05465709590936608, 0.05490128248826124, 0.05495215469219773, 0.05495724191259138, 0.054794450859994605, 0.051828601370497177, 0.051828601370497177, 0.05179807804813528]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
pure train time  2.5260913372039795
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 17.55810546875 GB
    Memory Allocated: 0.5166459083557129  GigaBytes
Max Memory Allocated: 15.331234455108643  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.0871553421020508
epoch  10
load pickle file time  0.5072364807128906
the output layer 
self.num_batch (get_in_degree_bucketing) 18
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  18
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.11366748809814453
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  18
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0004856586456298828
self.weights_list  [0.0551251201855818, 0.061402750151344804, 0.05572032497163874, 0.055109858524400854, 0.05862004059601874, 0.05975957796419614, 0.060197078918049964, 0.05480971252117556, 0.05479953808038825, 0.05473849143566447, 0.05465709590936608, 0.05490128248826124, 0.05495215469219773, 0.05495724191259138, 0.054794450859994605, 0.051828601370497177, 0.051828601370497177, 0.05179807804813528]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.012090921401977539
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11429882049560547
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  3.3766696453094482
self.buckets_partition() spend  sec:  0.12652945518493652
input layer
dataloader gen time  13.494309663772583
weights_list [0.0551251201855818, 0.061402750151344804, 0.05572032497163874, 0.055109858524400854, 0.05862004059601874, 0.05975957796419614, 0.060197078918049964, 0.05480971252117556, 0.05479953808038825, 0.05473849143566447, 0.05465709590936608, 0.05490128248826124, 0.05495215469219773, 0.05495724191259138, 0.054794450859994605, 0.051828601370497177, 0.051828601370497177, 0.05179807804813528]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
pure train time  2.538180351257324
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 17.55810546875 GB
    Memory Allocated: 0.5145530700683594  GigaBytes
Max Memory Allocated: 15.331234455108643  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.0042855739593506
epoch  11
load pickle file time  0.47217726707458496
the output layer 
self.num_batch (get_in_degree_bucketing) 18
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  18
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.11467671394348145
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  18
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0006067752838134766
self.weights_list  [0.0551251201855818, 0.061402750151344804, 0.05572032497163874, 0.055109858524400854, 0.05862004059601874, 0.05975957796419614, 0.060197078918049964, 0.05480971252117556, 0.05479953808038825, 0.05473849143566447, 0.05465709590936608, 0.05490128248826124, 0.05495215469219773, 0.05495724191259138, 0.054794450859994605, 0.051828601370497177, 0.051828601370497177, 0.05179807804813528]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.011118173599243164
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11545443534851074
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  3.4016432762145996
self.buckets_partition() spend  sec:  0.12659049034118652
input layer
dataloader gen time  13.560254573822021
weights_list [0.0551251201855818, 0.061402750151344804, 0.05572032497163874, 0.055109858524400854, 0.05862004059601874, 0.05975957796419614, 0.060197078918049964, 0.05480971252117556, 0.05479953808038825, 0.05473849143566447, 0.05465709590936608, 0.05490128248826124, 0.05495215469219773, 0.05495724191259138, 0.054794450859994605, 0.051828601370497177, 0.051828601370497177, 0.05179807804813528]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
pure train time  2.5113894939422607
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 17.55810546875 GB
    Memory Allocated: 0.5187244415283203  GigaBytes
Max Memory Allocated: 15.331234455108643  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.9583718180656433
epoch  12
load pickle file time  0.5699183940887451
the output layer 
self.num_batch (get_in_degree_bucketing) 18
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  18
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.10570120811462402
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  18
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0003643035888671875
self.weights_list  [0.0551251201855818, 0.061402750151344804, 0.05572032497163874, 0.055109858524400854, 0.05862004059601874, 0.05975957796419614, 0.060197078918049964, 0.05480971252117556, 0.05479953808038825, 0.05473849143566447, 0.05465709590936608, 0.05490128248826124, 0.05495215469219773, 0.05495724191259138, 0.054794450859994605, 0.051828601370497177, 0.051828601370497177, 0.05179807804813528]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.010243892669677734
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.10622000694274902
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  3.376002073287964
self.buckets_partition() spend  sec:  0.11648178100585938
input layer
dataloader gen time  13.459879159927368
weights_list [0.0551251201855818, 0.061402750151344804, 0.05572032497163874, 0.055109858524400854, 0.05862004059601874, 0.05975957796419614, 0.060197078918049964, 0.05480971252117556, 0.05479953808038825, 0.05473849143566447, 0.05465709590936608, 0.05490128248826124, 0.05495215469219773, 0.05495724191259138, 0.054794450859994605, 0.051828601370497177, 0.051828601370497177, 0.05179807804813528]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
pure train time  2.5469768047332764
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 17.55810546875 GB
    Memory Allocated: 0.5185084342956543  GigaBytes
Max Memory Allocated: 15.331234455108643  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.9159430265426636
epoch  13
load pickle file time  0.47562074661254883
the output layer 
self.num_batch (get_in_degree_bucketing) 18
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  18
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.11410140991210938
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  18
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0005018711090087891
self.weights_list  [0.0551251201855818, 0.061402750151344804, 0.05572032497163874, 0.055109858524400854, 0.05862004059601874, 0.05975957796419614, 0.060197078918049964, 0.05480971252117556, 0.05479953808038825, 0.05473849143566447, 0.05465709590936608, 0.05490128248826124, 0.05495215469219773, 0.05495724191259138, 0.054794450859994605, 0.051828601370497177, 0.051828601370497177, 0.05179807804813528]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.010622024536132812
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11474299430847168
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  3.4092304706573486
self.buckets_partition() spend  sec:  0.1253974437713623
input layer
dataloader gen time  14.016920566558838
weights_list [0.0551251201855818, 0.061402750151344804, 0.05572032497163874, 0.055109858524400854, 0.05862004059601874, 0.05975957796419614, 0.060197078918049964, 0.05480971252117556, 0.05479953808038825, 0.05473849143566447, 0.05465709590936608, 0.05490128248826124, 0.05495215469219773, 0.05495724191259138, 0.054794450859994605, 0.051828601370497177, 0.051828601370497177, 0.05179807804813528]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
pure train time  2.532210111618042
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 17.55810546875 GB
    Memory Allocated: 0.5174484252929688  GigaBytes
Max Memory Allocated: 15.331234455108643  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.875446617603302
epoch  14
load pickle file time  0.48633432388305664
the output layer 
self.num_batch (get_in_degree_bucketing) 18
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  18
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.11391305923461914
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  18
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0003726482391357422
self.weights_list  [0.0551251201855818, 0.061402750151344804, 0.05572032497163874, 0.055109858524400854, 0.05862004059601874, 0.05975957796419614, 0.060197078918049964, 0.05480971252117556, 0.05479953808038825, 0.05473849143566447, 0.05465709590936608, 0.05490128248826124, 0.05495215469219773, 0.05495724191259138, 0.054794450859994605, 0.051828601370497177, 0.051828601370497177, 0.05179807804813528]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.009881973266601562
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.1144413948059082
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  3.392749786376953
self.buckets_partition() spend  sec:  0.12434816360473633
input layer
dataloader gen time  13.246986150741577
weights_list [0.0551251201855818, 0.061402750151344804, 0.05572032497163874, 0.055109858524400854, 0.05862004059601874, 0.05975957796419614, 0.060197078918049964, 0.05480971252117556, 0.05479953808038825, 0.05473849143566447, 0.05465709590936608, 0.05490128248826124, 0.05495215469219773, 0.05495724191259138, 0.054794450859994605, 0.051828601370497177, 0.051828601370497177, 0.05179807804813528]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
pure train time  2.5360047817230225
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 17.55810546875 GB
    Memory Allocated: 0.515465259552002  GigaBytes
Max Memory Allocated: 15.331234455108643  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.8337762355804443
epoch  15
load pickle file time  0.5264103412628174
the output layer 
self.num_batch (get_in_degree_bucketing) 18
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  18
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.11402225494384766
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  18
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0006663799285888672
self.weights_list  [0.0551251201855818, 0.061402750151344804, 0.05572032497163874, 0.055109858524400854, 0.05862004059601874, 0.05975957796419614, 0.060197078918049964, 0.05480971252117556, 0.05479953808038825, 0.05473849143566447, 0.05465709590936608, 0.05490128248826124, 0.05495215469219773, 0.05495724191259138, 0.054794450859994605, 0.051828601370497177, 0.051828601370497177, 0.05179807804813528]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.010645151138305664
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11485147476196289
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  3.4045886993408203
self.buckets_partition() spend  sec:  0.125518798828125
input layer
dataloader gen time  13.372517108917236
weights_list [0.0551251201855818, 0.061402750151344804, 0.05572032497163874, 0.055109858524400854, 0.05862004059601874, 0.05975957796419614, 0.060197078918049964, 0.05480971252117556, 0.05479953808038825, 0.05473849143566447, 0.05465709590936608, 0.05490128248826124, 0.05495215469219773, 0.05495724191259138, 0.054794450859994605, 0.051828601370497177, 0.051828601370497177, 0.05179807804813528]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
pure train time  2.5354340076446533
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 17.55810546875 GB
    Memory Allocated: 0.516932487487793  GigaBytes
Max Memory Allocated: 15.331234455108643  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.7988079190254211
epoch  16
load pickle file time  0.5018537044525146
the output layer 
self.num_batch (get_in_degree_bucketing) 18
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  18
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.10415530204772949
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  18
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.00040531158447265625
self.weights_list  [0.0551251201855818, 0.061402750151344804, 0.05572032497163874, 0.055109858524400854, 0.05862004059601874, 0.05975957796419614, 0.060197078918049964, 0.05480971252117556, 0.05479953808038825, 0.05473849143566447, 0.05465709590936608, 0.05490128248826124, 0.05495215469219773, 0.05495724191259138, 0.054794450859994605, 0.051828601370497177, 0.051828601370497177, 0.05179807804813528]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.011024951934814453
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.10470819473266602
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  3.4297847747802734
self.buckets_partition() spend  sec:  0.1157541275024414
input layer
dataloader gen time  13.450681924819946
weights_list [0.0551251201855818, 0.061402750151344804, 0.05572032497163874, 0.055109858524400854, 0.05862004059601874, 0.05975957796419614, 0.060197078918049964, 0.05480971252117556, 0.05479953808038825, 0.05473849143566447, 0.05465709590936608, 0.05490128248826124, 0.05495215469219773, 0.05495724191259138, 0.054794450859994605, 0.051828601370497177, 0.051828601370497177, 0.05179807804813528]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
pure train time  2.526318073272705
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 17.55810546875 GB
    Memory Allocated: 0.5169520378112793  GigaBytes
Max Memory Allocated: 15.331234455108643  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.7639614939689636
epoch  17
load pickle file time  0.48802781105041504
the output layer 
self.num_batch (get_in_degree_bucketing) 18
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  18
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.1149132251739502
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  18
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0004978179931640625
self.weights_list  [0.0551251201855818, 0.061402750151344804, 0.05572032497163874, 0.055109858524400854, 0.05862004059601874, 0.05975957796419614, 0.060197078918049964, 0.05480971252117556, 0.05479953808038825, 0.05473849143566447, 0.05465709590936608, 0.05490128248826124, 0.05495215469219773, 0.05495724191259138, 0.054794450859994605, 0.051828601370497177, 0.051828601370497177, 0.05179807804813528]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.010454416275024414
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.1155850887298584
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  3.459519147872925
self.buckets_partition() spend  sec:  0.12605714797973633
input layer
dataloader gen time  13.628847599029541
weights_list [0.0551251201855818, 0.061402750151344804, 0.05572032497163874, 0.055109858524400854, 0.05862004059601874, 0.05975957796419614, 0.060197078918049964, 0.05480971252117556, 0.05479953808038825, 0.05473849143566447, 0.05465709590936608, 0.05490128248826124, 0.05495215469219773, 0.05495724191259138, 0.054794450859994605, 0.051828601370497177, 0.051828601370497177, 0.05179807804813528]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
pure train time  2.5135200023651123
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 17.55810546875 GB
    Memory Allocated: 0.5203442573547363  GigaBytes
Max Memory Allocated: 15.331234455108643  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.7317686676979065
epoch  18
load pickle file time  0.5643541812896729
the output layer 
self.num_batch (get_in_degree_bucketing) 18
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  18
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.1141512393951416
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  18
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.0006508827209472656
self.weights_list  [0.0551251201855818, 0.061402750151344804, 0.05572032497163874, 0.055109858524400854, 0.05862004059601874, 0.05975957796419614, 0.060197078918049964, 0.05480971252117556, 0.05479953808038825, 0.05473849143566447, 0.05465709590936608, 0.05490128248826124, 0.05495215469219773, 0.05495724191259138, 0.054794450859994605, 0.051828601370497177, 0.051828601370497177, 0.05179807804813528]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.009661674499511719
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11494159698486328
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  3.4003617763519287
self.buckets_partition() spend  sec:  0.12462186813354492
input layer
dataloader gen time  13.511207342147827
weights_list [0.0551251201855818, 0.061402750151344804, 0.05572032497163874, 0.055109858524400854, 0.05862004059601874, 0.05975957796419614, 0.060197078918049964, 0.05480971252117556, 0.05479953808038825, 0.05473849143566447, 0.05465709590936608, 0.05490128248826124, 0.05495215469219773, 0.05495724191259138, 0.054794450859994605, 0.051828601370497177, 0.051828601370497177, 0.05179807804813528]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
pure train time  2.5371739864349365
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 17.55810546875 GB
    Memory Allocated: 0.5145931243896484  GigaBytes
Max Memory Allocated: 15.331234455108643  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.7025768756866455
epoch  19
load pickle file time  0.5410170555114746
the output layer 
self.num_batch (get_in_degree_bucketing) 18
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  18
type of fanout_dst_nids  <class 'torch.Tensor'>
sum(estimated_mem)
27.023624836713726
24
the last batch value is  1028
G_BUCKET_ID_list [[22], [6, 5, 3, 1], [21, 0], [23], [13, 9], [14, 7, 2], [10, 8, 4], [20], [18], [19], [16], [17], [15], [12], [11]]
G_BUCKET_ID_list length 15
backpack scheduling spend  0.11375069618225098
len(g_bucket_nids_list)  15
len(local_split_batches_nid_list)  18
current group_mem  2.232189497508864
current group_mem  1.3068242388952873
current group_mem  2.2282285622209037
current group_mem  2.223171411479282
current group_mem  2.2199646769036976
current group_mem  2.2155376024494022
current group_mem  2.1752215580623586
current group_mem  1.920127067184496
current group_mem  1.793769060001223
current group_mem  1.6766132844613435
current group_mem  1.6565154083265854
current group_mem  1.5991467489715145
current group_mem  1.4645761989081698
current group_mem  1.2832128918560093
current group_mem  1.0285266294845892
batches output list generation spend  0.00038433074951171875
self.weights_list  [0.0551251201855818, 0.061402750151344804, 0.05572032497163874, 0.055109858524400854, 0.05862004059601874, 0.05975957796419614, 0.060197078918049964, 0.05480971252117556, 0.05479953808038825, 0.05473849143566447, 0.05465709590936608, 0.05490128248826124, 0.05495215469219773, 0.05495724191259138, 0.054794450859994605, 0.051828601370497177, 0.051828601370497177, 0.05179807804813528]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.011324644088745117
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.11427545547485352
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  3.395066738128662
self.buckets_partition() spend  sec:  0.12561869621276855
input layer
dataloader gen time  13.55898642539978
weights_list [0.0551251201855818, 0.061402750151344804, 0.05572032497163874, 0.055109858524400854, 0.05862004059601874, 0.05975957796419614, 0.060197078918049964, 0.05480971252117556, 0.05479953808038825, 0.05473849143566447, 0.05465709590936608, 0.05490128248826124, 0.05495215469219773, 0.05495724191259138, 0.054794450859994605, 0.051828601370497177, 0.051828601370497177, 0.05179807804813528]
step  0
step  1
step  2
step  3
step  4
step  5
step  6
step  7
step  8
step  9
step  10
step  11
step  12
step  13
step  14
step  15
step  16
step  17
pure train time  2.5797886848449707
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 17.55810546875 GB
    Memory Allocated: 0.5133819580078125  GigaBytes
Max Memory Allocated: 15.331234455108643  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.6743553876876831
Total (block generation + training)time/epoch 20.039727210998535
pure train time/epoch 2.5334796458482742
dataloader time  [15.185631513595581, 14.429777383804321, 13.99072265625, 14.331257343292236, 13.897758483886719, 13.819269180297852, 13.254985570907593, 13.494309663772583, 13.560254573822021, 13.459879159927368, 14.016920566558838, 13.246986150741577, 13.372517108917236, 13.450681924819946, 13.628847599029541, 13.511207342147827, 13.55898642539978]
dataloader time avg per epoch 13.559431057709913

num_input_list  [15984581, 15979011, 15980337, 15981176, 15984122, 15976489, 15985905, 15980842, 15983205, 15978845, 15978751, 15988129, 15979901, 15973862, 15973558, 15968283, 15975877, 15975440, 15981015, 15976959]
